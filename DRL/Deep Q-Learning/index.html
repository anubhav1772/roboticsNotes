<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta http-equiv="X-UA-Compatible" content="ie=edge">
      <title>Deep Q-Learning</title>
      <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
      <link rel="stylesheet" href="../../assets/css/plyr.css">
      <link rel="stylesheet" href="../../assets/css/katex.min.css">
      <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
      <link rel="stylesheet" href="../../assets/css/styles.css">
      <link rel="stylesheet" href="../../assets/css/cc-icons.min.css">    <!-- Creative Commons Icons -->
      <link rel="shortcut icon" type="image/png" href="../../assets/img/robo-icon.png" />
      <style type="text/css">
         /* Three image containers (use 25% for four, and 50% for two, etc) */
         .column {
         float: left;
         width: 33.33%;
         padding: 5px;
         }
         /* Clear floats after image containers */
         .row::after {
         content: "";
         clear: both;
         display: table;
         }
      </style>
   </head>
   <body>
      <div class="wrapper">
      <nav id="sidebar">
         <div class="sidebar-header">
            <h3>Deep Q-Learning</h3>
         </div>
         <ul class="sidebar-list list-unstyled CTAs">
            <li>
               <a href="../../index.html" class="article">Back to Home</a>
            </li>
         </ul>
         <ul class="sidebar-list list-unstyled components">
            <li class="">
               <a href="#">01. Intro to Deep Q-Learning</a>
            </li>
            <li class="">
               <a href="#">02. Neural Nets as Value Functions</a>
            </li>
            <li class="">
               <a href="#">03. Monte Carlo Learning</a>
            </li>
            <li class="">
               <a href="#">04. Temporal Difference Learning</a>
            </li>
            <li class="">
               <a href="#">05. Q-Learning</a>
            </li>
            <li class="">
               <a href="#">06. Deep Q Network</a>
            </li>
            <li class="">
               <a href="#">07. Experience Replay</a>
            </li>
            <li class="">
               <a href="#">08. Fixed Q Targets</a>
            </li>
            <li class="">
               <a href="#">09. Deep Q-Learning Algorithm</a>
            </li>
            <li class="">
               <a href="#">10. DQN Improvements</a>
            </li>
         </ul>
         <ul class="sidebar-list list-unstyled CTAs">
            <li>
               <a href="../../index.html" class="article">Back to Home</a>
            </li>
            <li>
               <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" class="article"><i class="cc cc-SIX cc-4"></i></a>
            </li>
         </ul>
      </nav>
      <div id="content">
         <header class="container-fluild header">
            <div class="container">
               <div class="row">
                  <div class="col-12">
                     <div class="align-items-middle">
                        <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                           <div></div>
                           <div></div>
                           <div></div>
                        </button>
                        <h1 style="display: inline-block">Deep Q-Learning</h1>
                     </div>
                  </div>
               </div>
            </div>
         </header>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-td0">Intro to Deep Q-Learning</h2>
            </div>
            <!-- <div class="row">
               <div class="column">
                 <img src="img/dql1.png" alt="dql1" style="width:100%">
               </div>
               </div> -->
            <p>Deep Q-Learning is an algorithm that demonstrates how we can use neural networks to solve RL problems. It has expanded the range of domain we can tackle especially those with large and continuous state spaces.</p>
            <ul>
               <li>How can neural networks be used to represent value functions?</li>
               <li>We will adapt the two main classes of model-free approaches (MC and TD Learning) to work with this new representation.</li>
               <li>Deep Q-Learning (Remember Q-Learning is a variant of TD Learning)</li>
               <li>How can we harness the power of deep learning, including CNNs and RNNs, to teach computers tasks that seem almost impossible to learn from scratch?</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Neural Nets as Value Functions</h2>
               <p>Neural networks (NNs) are considered universal function approximators, capable of approximating any continuous function to a desired degree of accuracy.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql2.png" alt="dql2" style="width:100%">
               </div>
            </div>
            <p>What if we use neural networks to represent our value function?</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql3.png" alt="dql3" style="width:100%">
               </div>
            </div>
            <p>A state value function maps any state to a real number which indicates how valuable that state is according to the current policy <span class="mathquill ud-math">\pi</span>. If we a neural network to approximate this function then the input would need to be fed in as a vector. We already know how to do this using a feature transformation X(s). Now the input can progress through the network and if it is designed to ouput a single real number, that's our value as estimated by the network. This seems to fall in place with the general idea of approximating a value function with the weights of the neural network forming the parameter vector w. The question is <I>how do we learn these parameters?</I></p>
            <div class="row">
               <div class="column">
                  <img src="img/dql4.png" alt="dql4" style="width:100%">
               </div>
            </div>
            <p>If we have a reference or target we are trying to reach, for example <span class="mathquill ud-math">v_{\pi}(s)</span>, then we can use the square difference between the estimated and target value as our error or loss. Then we can backpropagate it through the network adjusting weights along the way to minimize loss. One popular method for adjusting weights is <strong>gradient descent</strong>, where we iteratively change weights a small step away from the direction of error. In order to apply the gradient descent, we need to know the derivative of the value function represented by the network with respect to its weights. This can become very complex, especially for networks with deep architectures but we have pretty efficient algorithms implemented in libraries like tensorflow, pytorch, theano and mxnet to train neural nets for us. All we need is to figure out the loss. That's where our knowledge of RL comes in.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql5.png" alt="dql5" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql6.png" alt="dql6" style="width:100%">
               </div>
            </div>
            <p>At this point, let's also consider the action value function <span class="mathquill ud-math">q_{\pi}(s, a)</span>. The update rule looks very similar to what we had for state value function <span class="mathquill ud-math">v_{\pi}(s)</span>. But we have the same problem here. In most practical problems, there is oracle to tell us what's the correct value function <span class="mathquill ud-math">v_{\pi}(s)</span> or <span class="mathquill ud-math">q_{\pi}(s, a)</span> should be. We need to use a more realistic target, one that is based on our interaction with the environement. This is where RL fundamentally differs from a supervised learning.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Monte Carlo Learning</h2>
               <p>Recall the incremental step that is used in the classical MC learning to update value functions.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql7.png" alt="dql7" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql8.png" alt="dql8" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql9.png" alt="dql9" style="width:100%">
               </div>
            </div>
            <p>Here <span class="mathquill ud-math">G_{t}</span> is the return, i.e., the cummulative discounted reward received following time t. That's a suitable target to attain. So, let's take our neural network update rule and simply substitute the unknown true value function with this return. This gives a concrete update rule for state value functions represented by neural nets or other function approximators. We can do the same for action value functions as well.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql10.png" alt="dql10" style="width:100%">
               </div>
            </div>
            <p>Now that we have the main building block, the update rule, let's build a complete MC algorithm around that. Let's focus on the control problem and adapt our classical MC algorithm to work with a function approximator. It generally includes an evaluation step where we try to estimate the value of each (state, action) pair under the current policy. We do this by interacting with the environment to generate an episode using the policy <span class="mathquill ud-math">\pi</span> and then for each time step t in the episode we update the parameter vector w using the (state, action) pair <span class="mathquill ud-math">(S_{t}, A_{t})</span> and the return <span class="mathquill ud-math">G_{t}</span> computed from the remainder of the episode. This is followed by an improvement step where we extract an ε-greedy policy based on these Q-values. At the beginning we need to initialize our parameter w with random values and start with a policy <span class="mathquill ud-math">\pi</span> defined in the same ε-greedy manner. Then we can repeat these two steps over and over till the weights converge, resulting in the optimal value function and hence the corresponding policy. Note that this is the every visit version of MC. For the first visit version, we only perform the weight update when we see the (state, action) pair for the first time in an episode. </p>
            <p>MC is guaranteed to converge on a local optimum in general; in case of a linear function approximation, it will converge on the global optimum.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Temporal Difference Learning</h2>
               <p>Our second strategy will be to develop a TD technique with function approximation.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql11.png" alt="dql11" style="width:100%">
               </div>
            </div>
            <p>Compare the incremental update step between MC learning where we used the actual return obtained through the episode with the TD case where we use an estimated return. This is called the TD target and in the simplest case TD(0) we use the next reward plus the discounted value of the next state.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql12.png" alt="dql12" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql13.png" alt="dql13" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql14.png" alt="dql14" style="width:100%">
               </div>
            </div>
            <p>In a similar manner as before, we can use this TD target in place of our unknown true value function. This gives us something concrete to work with. Note that we had to adapt the value function to use our function approximator <span class="mathquill ud-math">\hat{V}</span>. This entire difference is called the TD Error and denoted by <span class="mathquill ud-math">δ_{t}</span>. We can extend the same idea to the action value functions as well. Now we are ready to build an algorithm around this update rule!</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql15.png" alt="dql15" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql16.png" alt="dql16" style="width:100%">
               </div>
            </div>
            <p>We will use a TD(0) target and focus on the control problem again. This is esentially the SARSA algorithm. We will begin by initializing parameter w arbitrarily. Our policy <span class="mathquill ud-math">\pi</span> is defined as an ε-greedy choice over our approximate action value function <span class="mathquill ud-math">\hat{q}</span>. Then we start interacting with the environment. For each episode we begin with some initial state S obtained from the environment. We continue interacting till we reach a terminal state. At each time step, we choose an action A to perform and observe the reward R and next state S'. Now we choose another action A' (this time from state S') according to our ε-greedy policy <span class="mathquill ud-math">\pi</span>. This gives us all we need for the SARSA update, (S, A, R, S', A'). We plug these into our gradient descent update rule and adjust our weights accordingly. Finally we simply roll over S' to be the new S, A' to be the new A and repeat. This formulation is useful for episodic tasks where each episode is guaranteed to terminate. It can be adapted to continuing tasks by eliminating the distinct boundary between episodes and treating the sequence of interactions as one long unending episode.</p>
            <p><strong>SARSA is an on-policy algorithm</strong> which means that we are updating the same policy that we are following to carry out actions. This usually works very well and converges pretty quickly because we are using the most updated policy to take each decision. But, this also has some drawbacks! Mainly that the policy being learned and one being followed are intimately tied to each other. What if we wanted to follow one policy, say one that is more exploratory while learning the more optimal policy? That's where we need off-policy algorithms.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Q-Learning</h2>
               <p>Q-Learning is an off-policy variant of TD Learning.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql17.png" alt="dql17" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql18.png" alt="dql18" style="width:100%">
               </div>
            </div>
            <p>Just like SARSA, we initialize w arbitatrily and define an ε-greedy policy <span class="mathquill ud-math">\pi</span> based on the Q-values. Over multiple episodes we use this ε-greedy policy to repeatedly take an action and observe the reward and the next state. The main difference is in the update step. Instead of picking the next action from the same ε-greedy policy, we choose an action greedily which would maximize the expected value going forward. Note that we don't actually take this action, it is only used for performing the update. In fact, we don't even need to pick this action, we can simply use the max Q-value for the next state. This is why, Q-Learning is considered as an off-policy method. We use one policy to take actions (ε-greedy policy <span class="mathquill ud-math">\pi</span>) and yet another policy to perform value updates (a greedy policy). Although both of them are defined on same underlying Q-values, these two are indeed different policies. This is how the Q-Learning looks like for an episodic task. We can use the same algorithm for continuing tasks by treating the whole unending sequence as one long episode or modify it slightly to remove the concept of episodes. Both forms are equivalent. In either case, we may need some additional criteria to figure out when we have fully learned the task or to detect if we are failing measurably. </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql19.png" alt="dql19" style="width:100%">
               </div>
            </div>
            <p>SARSA is an on-policy algorithm that follows the same policy it is learning. In general, it is more suitable for online learning as at any point of time we are using the most updated policy as per our interactions with with the environment. However, it also means that if we use ε-greedy action selection which is necessary to encourage exploration then this randomness also affects the Q-values that are learned. </p>
            <p>On the other hand, Q-Learning is an off-policy method meaning the policy it follows to choose an action is different from the policy it's learning. This can lead to bad online performance because there is a disconnect between the policy we are learning and the one we are following. But the good thing is that the ε-greedy nature of action selection doesn't impact the Q-values learnt. </p>
            <p>Thus, the two appraches have their pros and cons and which algorithm we choose will depend on the characteristics of the environement and our preferences regarding online performance versus more accurate learning.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql20.png" alt="dql20" style="width:100%">
               </div>
            </div>
            <p>The biggest reason off-policy methods like Q-Learning have got so much attention is that they decouple the actions an agent takes in the environment from its learning process. That gives us the opportunity to build various different variations of our learning algorithm. For instance, we can use a more exploratory policy while acting and yet learned the optimal value function. Yes, the online performance will be bad but at some point we can stop exploring and follow the optimal policy for better results. In fact, the policy used to take actions need not be the agent's own. A human can demonstrate what actions to take and the agent can learn from observing the effects of those actions. It also makes it easier to learn offline or in batches since an update to the polcy need not be performed at every step. This as we will see is critical for a lively training neural networks for RL.  </p>
            <p>One drawback of both SARSA & Q-Learning, since they are TD approaches, is that they may not converge on the global optimum when using non-linear function approximation.
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Deep Q Network</h2>
               <p>True to its name, at the heart of the agent is a deep neural network that acts as a function approximator.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql21.png" alt="dql21" style="width:100%">
               </div>
            </div>
            <p>We pass images from our favourite video game one screen at a time and it produces a vector of action values, with the max value indicating the action to take. As a reinforcement signal it is fed back the change in game score at each time step. In the beginning when the neural network is initialized with random values, the actions taken are all over the place. It's really bad as we would expect. But over time, it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well. </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql22.png" alt="dql22" style="width:100%">
               </div>
            </div>
            <p>Consider how complex the input space is, Atari game is displayed at a resolution of 210 X 160 pixels with 128 possible colors for each pixel. This is still technically a discrete state space but very large to process as is. To reduce this complexity, the Deepmind team decided to perform some minimal processing like convert the frames to grayscale and scale them down to a square 84 X 84 pixel block. Square images allowed them to use more optimized neural network operations on GPUs. In order to give the agent access to a sequence of frames they stacked four such frames together resulting in a finite state space size of 84 X 84 X 4. There might be other approaches to dealing with sequential data but this was a simple approach that seem to work pretty well.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql23.png" alt="dql23" style="width:100%">
               </div>
            </div>
            <p>On the output side, unlike a traditional RL setup where only one Q-value is produced at a time, the Deep Q-Network is designed to produce a Q-value for every possible action in a single forward pass. Without this, we would have to run the network individually for every action. Instead we can simply use this vector to take an action, either stochastically or by choosing the one with the maximum value.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql24.png" alt="dql24" style="width:100%">
               </div>
            </div>
            <p>These innovative input and output transformations support a powerful yet simple neural network architecture under the hood. The screen images are first processed by convolutional layers. This allows the system to exploit the spatial relationships and can exploit spatial space. Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames. The original DQN agent uses three such convolutional layers with relu activation. They were followed by one fully-connected hidden layer with relu action and one fully-connected linear output layer that produce the vector of action values. This same architecture was used for all the atari games they tested on but each game was learned from scratch with a freshly initialized network.</p>
            <p>Training such networks require a lot of data but even then it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge due to the high correlation between between actions and states. This can result in a very unstable and ineffective policy. In order to overcome these challenges, the researchers came up with several techniques that slightly modified the base Q-Learning algorithm. We will take a look at two of these techniques.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql25.png" alt="dql25" style="width:100%">
               </div>
            </div>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Mnih et al., 2015. <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank">Human-level control through deep reinforcement learning</a>.</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <h3></h3>
            <div>
               <h2 id="-td-prediction-action-values">Experience Replay</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql26.png" alt="dql26" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql27.png" alt="dql27" style="width:100%">
               </div>
            </div>
            <p>The idea of experience replay and its application of training neural networks for RL isn't new. It was originally proposed to make more efficient use of observed experiences. Consider the basic online Q-Learning algorithm where we interact with the envieonment and at each time step, we obtain a (state, action, reward, next state) tuple, learn from it and then discard it, moving on to the next tuple in the following time step. This seems a little wasteful. We could possibly learn more from these experience tuples if we store them somewhere. Moreover some states are pretty rare to come by and some actions can be pretty costly. So, it would be nice to recall such experiences. That is exactly what a <strong>replay buffer</strong> allows us to do.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql28.png" alt="dql28" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql29.png" alt="dql29" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30.png" alt="dql30" style="width:100%">
               </div>
            </div>
            <p>We store each experience tuple in this buffer as we are interacting with the environment and then sample a small batch of tuples from it in order to learn. As a result, we are able to learn from individual tuples multiple times, recall rare occurences and in general, make better use of our experience. But there is an another critical problem that the experience replay can help with and this is what DQN takes advantage of. If we think about the expeiences being obtained, we will realize that every action <span class="mathquill ud-math">A_{t}</span> affects the next state <span class="mathquill ud-math">S_{t}</span> in some way which means that a sequence of expeience tuples can be highly correlated. A naive Q-Learning approach that learns from each of these expeiences in sequantial order runs the risk of getting swayed by the effect of this correlation. With experience replay we can sample from this buffer at random. It doesn't have to in the same sequence as we stored the tuples. This helps break the correlation and ultimately prevents action values from oscillating or diverging catastrophically.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql30-1.png" alt="dql30-1" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30-2.png" alt="dql30-2" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30-3.png" alt="dql30-3" style="width:100%">
               </div>
            </div>
            <p>Let's look at an example. I am learning to play tennis and I like to practice rallying against the wall. I am more confident with my forehand shot than my backhand shot and I can hit the ball fairly straight. So the ball keeps coming back around the same area to my right and I keep hitting forehand shots. If I were an online Q-Learning agent, learning as I play, this is what I might pick up. When the ball comes to my right, I should hit with my forehand less certainly at first but with increase in confidence as I repeatedly hit the ball. Great! I am learning to play forehand pretty well. But I am not exploring rest of the state space. This could be addressed by using an ε-greedy policy acting randomly with a small chance. So I try different combinations of states and actions and sometimes I make mistakes but I eventually figure out the best overall policy. Use a forehand shot when the ball comes to my right and backhand when it comes to my left. This learning strategy seems to work with a Q table where we have assumed this highly simplified state space with just two discrete states. But when we consider a continuous state space, things can fall apart. First, the ball can come anywhere between the extreme left and extreme right. If I try to discretized this range into small buckets, I will have too many possibilities. What if I endup learning a policy with too may holes in it, states or situations that I maynot have visited during practice. In fact, it makes more sense to use a function approximator like a linear combination of RBF kernels or a Q network that can generalize my learning across the space. Now everytime the ball comes to my right and I successfully hit a forehand shot, my value function changes slightly. It becomes more positive around the exact region where the ball came but also raises the value of forehand shot, in general, across the state space. The effect is less pronounce away from the exact spot but over time it can add up. And that's exactly what happens when I tried to learn while playing processing each experience tuple in order. For instance, if my forehand shot is farely straight, I likely get back the ball around the same spot. This produces a state pretty similar to the previous one, so I use my forehand again and if it is successful, it reinforces my belief that forehand is a good choice. I can easily get trapped in the cycle. Ultimately, if I don't see too many examples of ball coming to my left for a while, then the value of forehand shot can become greater than back hand across the entire state space. My policy would be then to choose forehand regardless of where I see the ball coming. Disaster! <em>How can we fix this?</em> </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql30-4.png" alt="dql30-4" style="width:100%">
               </div>
            </div>
            <p>The first thing I should do is to stop learning while practicing. This time is best spend in trying out different shots playing a little randomly and thus exploring the state space. It then becomes important to remember my interactions, what shots worked well in what situation, etc. When I take a break or when I am back home and resting, that's a good time to recall these experiences and learn from them. The main advantage is that now I have a more comprehensive set of examples, somewhere the ball came to my right, somewhere it cam to my left, some forehand shots and some backhand.  I can generalize patterns from across these examples, recalling them in whatever order I please. This helps me avoid being fixated on one region of the state space or reinforcing the same action over and over. After a round of learning, I can go back to playing with my updated value function, again collect a bunch of experiences and then learn from them in a batch. In this way, experience replay can help us learn a more robust policy, one that is not affected by the inherent correlation present in the sequence of observed experience tuples.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql31.png" alt="dql31" style="width:100%">
               </div>
            </div>
            <p>If we think about it, this approach is basically building a database of samples and then learning a mapping from them. In that sense, expeience replay helps us to reduce the RL problem or atleast the value learning portion of it into a supervised learning scenario. We can then apply other models, learning techniques and best practices developed in the supervised learning literature to RL. We can even improve upon this idea, for example by prioritizing experience tuples that are rare or more important. </p>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Long-Ji Lin, 1993. <a href="https://pdfs.semanticscholar.org/54c4/cf3a8168c1b70f91cf78a3dc98b671935492.pdf" target="_blank">Reinforcement learning for robots using neural networks</a>.</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Fixed Q Targets</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql32.png" alt="dql32" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql33.png" alt="dql33" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql33-1.png" alt="dql33-1" style="width:100%">
               </div>
            </div>
            <p>Experience Replay helps us address one type of correlation, i.e., between consecutive experience tuples. There is another kind of correlation that Q-Learning is susceptible to. Q-Learning is a form of TD Learning. Here, <span class="mathquill ud-math">R+\gamma \max_{a} \hat{q}(S', a, w)</span> (<span class="mathquill ud-math">\max_{a} \hat{q}= </span>maximum possible value from the next state) is called the TD Target and our goal is to reduce the difference between this target and the currently predicted Q-value. This difference is the TD Error. The TD Target here is supposed to be a replacement for the true value function <span class="mathquill ud-math">q_{\pi}(S, A)</span> which is unknown to us.</p>
            <p>We originally used to <span class="mathquill ud-math">q_{\pi}</span> to define a squared error loss and differentiated that w.r.t. w to get our gradient descent update rule. Now <span class="mathquill ud-math">q_{\pi}</span> is not dependent on our function approximation or its parameters, thus resulting in a simple derivative and update rule. But, our TD Target is dependent on these parameters which means simply replacing the true value <span class="mathquill ud-math">q_{\pi}</span> with a target like this is mathematically incorrect.</p>
            <p>We can get away with it in practice because every update results in a small change to the parameters which is generally in the right direction. If we set <span class="mathquill ud-math">\alpha = 1</span> and leap towards the target, then we would likely overshoot and land in the wrong place. Also this is less of a concern when we use a lookup table or dictionary since Q-values are stored separately for each (state, action) pair. But it can affect learning significantly when we use function approximation where all the Q-values are intrinsically tied together through the function parameters. <em>Doesn't experience replay take care of this problem?</em> Well, it addresses a similar but a slightly different issue. There we broke the correlation between consecutive experience tuples by sampling them randomly out of order. Here the correlation is between the target and the parameters we are changing. This is like chasing a moving target. Literally!</p>
            <p>It's like trying to train a donkey to walk straight by sitting on it and dangling a carrot in front. Yes, the donkey might step forward and the carrot usually gets further away, always staying a little out of reach. But, contrary to popular belief this doesn't quite work as we would expect. The carrot is much more likely to bounce around randomly throwing the donkey off with every jerky step. Each action affects the next position of the target in a very complicated and unpredictable manner. We shouldn't be surprised if the donkey gets frustrated jumping around the spot and gives up. Instead we should get off the donkey, stand in one place and dangle the carrot from there. Once the donkey reaches that spot, move a few steps ahead dangle another carror and repeat. What we are essentially doing is decoupling the target position from the donkey's actions giving a more stable learning environment.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql34.png" alt="dql34" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql35.png" alt="dql35" style="width:100%">
               </div>
            </div>
            <p>We can do pretty much the same thing in Q-Learning by fixing the function parameters used to generate our target. The fixed parameters indicated by <span class="mathquill ud-math">w^{-}</span> are basically a copy of w that we don't change during the learning step. In practice, we copy <span class="mathquill ud-math">w</span> into <span class="mathquill ud-math">w^{-}</span>, use it generate the target while changing <span class="mathquill ud-math">w</span> for a certain number of learning steps. Then we update <span class="mathquill ud-math">w^{-}</span> with the latest <span class="mathquill ud-math">w</span>, again learn for a number of steps and so on. This decouples the target from the parameters, makes the learning algorithm much more stable and less likely to diverge or fall into oscillations. </p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Deep Q-Learning Algorithm</h2>
            </div>
            <!-- <div class="row">
               <div class="column">
                 <img src="img/dql36.png" alt="dql36" style="width:100%">
               </div>
               </div> -->
            <img src="img/dql36.png" style="width:60%" alt="" class="img img-fluid" /></br>
            <p>There are two main processes that are interleaved in this algorithm, one is where we sample the environment by performing actions and store away the observed experience tuples in the replay memory. The other is where we select a small batch of tuples from this memory randomly and then learn from that batch using a gradient descent update step.</p>
            <p>These two processes are not directly dependent on each other, so we could perform multiple sampling steps, then one learning step or even multiple learning steps with different random batches. The rest of the algorithm is designed to support these steps.</p>
            <p>In the bginning, we need to initialize an empty replay memory D. Note that the memory is finite, so we may want to use something like a circular queue that retains the n most recent experience tuples. Then we also need to initialize the parameters and weights of our neural network. There are certain best practices that we can use, for instance, sample the weights randomly from a normal distribution with variance equal to 2/(number of inputs to each neuron). These initialization methods are typically available in modern DL libraries like tensorflow, pytorch, keras, etc. so we won't need to implement them ourself.</p>
            <p>To use the Fixed Q-Targets technique, we need a second set of parameters <span class="mathquill ud-math">w^{-}</span> which we can initialize to <span class="mathquill ud-math">w</span>. Now remember this specific algorithm was designed to work with video games, so for each episode and each time step t within that episode we observe a raw screen image or input frame <span class="mathquill ud-math">x_{t}</span> which we need to convert to grayscale, crop to square size, etc. Also, in order to capture temporal relationship, we can stack a few input frames to build each state vector. Let's denote this preprocessing and stacking operation by function <span class="mathquill ud-math">\phi</span> which takes a sequence of frames and produce some combined representation. Note that if we want to stack, let's say 4 frames, then we would have to something special for the first three time steps. For instance, we can treat those missing frames as blank or just use copies of the first frame or we can just skit storing the experience tuples till we get a complete sequence. In practice, we won't be able to run the learning step immediately. We will need to wait till we have sufficient number of experience tuples in memory. Note that we don't clear out the memory after each episode. This enables us to recall and build batches of experiences from across episodes.</p>
            <p>There are many other techniques and optimizations that are used in DQN papers such as reward clipping, error clipping, storing past actions as part of the state vectors, dealing with terminal states, decaying <span class="mathquill ud-math">\epsilon</span> over time, etc. We may need to choose which technique we apply and adapt them different types of environment!</p>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Mnih et al., 2015. <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank">Human-level control through deep reinforcement learning</a>. (DQN paper)</li>
               <li>He et al., 2015. <a href="https://arxiv.org/abs/1502.01852" target="_blank">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>. (weight initialization)</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">DQN Improvements</h2>
            </div>
            <p>Several improvements to the original deep Q-Learning algorithm have been suggested. We will look at three of the most prominent ones.</p>
            <ul>
               <li>Double DQNs</li>
               <li>Prioritized Replay</li>
               <li>Dueling Networks</li>
            </ul>
            <!-- <div class="row">
               <div class="column">
                 <img src="img/dql37.png" alt="dql37" style="width:100%">
               </div>
               </div> -->
            <div class="row">
               <div class="column">
                  <img src="img/dql38.png" alt="dql38" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql39.png" alt="dql39" style="width:100%">
               </div>
            </div>
            <p>The first problem we are going to address is the overestimation of action values that Q-Learning is proned to. Let's look back at the update rule for Q-Learning with function approximation and focus on the TD Target. Here the max operation is necessary to find the best possible value we could get from the next state.</p>
            <p>To understand this better let's rewrite the target and expand the max operation. It's just a more efficient way of saying that we want to obtain the Q-value for the state <span class="mathquill ud-math">S^{′}</span> and the action that results in the maximum Q-value among all possible actions from that state. When we write it this way, we can see that it's possible for the argmax operation to make a mistake, especially in the early stages. <em>Why?</em> Because the Q-value is still evolving and we may not have gathered enough information to figure out the best action. The accuracy of our Q-values depends a lot on what actions have been tried and what neighboring states have been explored. In fact, it has been shown that this results in an overestimation of Q-values since we always pick the maximum among a set of noisy numbers. So, may be we shouldn't blindly trust these values. <em>What can we do to make our estimation robust?</em></p>
            <div class="row">
               <div class="column">
                  <img src="img/dql40.png" alt="dql40" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql41.png" alt="dql41" style="width:100%">
               </div>
            </div>
            <p>One idea that has been shown to work very well in practice is called <strong>Double Q-Learning</strong> where we select the best action using one set of parameters <span class="mathquill ud-math">w</span> but evaluate it using a different set of parameters <span class="mathquill ud-math">w^{′}</span>. It's basically like having two separate function approximators that must agree on the best action. If <span class="mathquill ud-math">w</span> picks an action that is not the best according to <span class="mathquill ud-math">w^{′}</span>, then the Q-value returned is not that high. In the long run, this prevents the algorithm from propagating incidental high rewards that may have obtained by chance and don't reflect long term returns. <em>Where do we get the second set of parameters from?</em></p>
            <p>In the original formulation of Double Q-Learning, we would basically maintain two value functions and randomly choose one of them to update at each step, using other only for evaluating actions. </p>
            <p>But when using <strong>DQNs with Fixed Q Targets</strong>, we already have an alternate set of parameters. Remember <span class="mathquill ud-math">w^{-}</span>! It turns out that since <span class="mathquill ud-math">w^{-}</span> is kept frozen for a while, it is different enough from <span class="mathquill ud-math">w</span> that it can be reused for this purpose.</p>
            <p>This simple modification keeps Q-value in check preventing them from exploding in early stages of learning or fluctuating later on. The resulting polciies have also been shown to perform significantly better than vanilla DQNs. </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql44.png" alt="dql44" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql45.png" alt="dql45" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql42.png" alt="dql42" style="width:100%">
               </div>
            </div>
            <p>The next issue we look at is related to experience replay. Recall the basic idea behind it.</p>
            <p>We interact with the environment to collect experience tuples, save them in a buffer and later we randomly sample a batch to learn from. This helps us break the correlation between consecutive experiences and stabilizes our learning algorithm. But, some of these experiences may be more important for learning than others. Moreover, these important experiences might occur infrequently. If we sample the batches uniformly, then these experiences have a very small chance of getting selected and since buffers are practically limited in capacity, older important experiences may get lost. This is where the idea of <strong>Prioritized Experience Replay</strong> comes in. <em>But what criteria should we used to assign priorities to each tuple?</em> </p>
            <p>One approach is to use <strong>TD Error</strong>, <span class="mathquill ud-math">\delta</span>. <U>The bigger the error, the more we expect to learn from that tuple.</U> So, let's take the magnitude of this error as a measure of priority and store it along with each corresponding tuple in the replay buffer. When creating batches we can use this value to compute a <strong>sampling probability</strong>. Select any tuple <span class="mathquill ud-math">i</span> with a probability equal to its priority value <span class="mathquill ud-math">p_{i}</span> normalised by the sum of all priority values in the replay buffer. And when a tuple is picked we can update its priority with a newly computed TD Error using the latest Q-values. This seems to work fairly well and has been shown to reduce the number of batch updates needed to learn a value function. </p>
            <p>There are a coouple of things we can improve. First, note that if the TD Error is 0, then the priority value of the tuple and hence its probability of being picked will also be 0. Zero or a very low TD Error doesn't necessarily mean we have nothing more to learn from such a tuple. It might be the case that our estimate was close due to the limited samples we visited till that point. So, to prevent such tuples from being starved for selection, we can add a small constant <span class="mathquill ud-math">e</span> to every priority value.</p>
            <p>Another issue along simila lines is that greedily using these priority values may lead to a small subset of experiences being replayed over and over, resulting in a sort of overfitting to that subset. To avoid this, we can reintroduce some element of uniform random sampling. This adds another hyperparameter <span class="mathquill ud-math">a</span> which we use to redefine the sampling probability as <span class="mathquill ud-math">p_{i}^{a}</span> divided by the sum of all priorities <span class="mathquill ud-math">p_{k}</span> each raised to the power <span class="mathquill ud-math">a</span> (i.e., <span class="mathquill ud-math">p_{k}^{a}</span>). We can control how much we want to use priorities versus randomness by varying this parameter where <span class="mathquill ud-math">a = 0</span> corresponds to pure uniform randomness and <span class="mathquill ud-math">a = 1</span> only uses priorities.</p>
            <p>When we use prioritized experience replay we have to make one adjustment to our update rule. Remember that our original Q-Learning update is derived from an expectation over all experiences. When using a stochastic update rule, the way we sample these experiences mus match the underlying distribution they came from. This is preserved when we sample experience tuples uniformly from the replay buffer. But, this assumption is violated when we use a non-uniform sampling, for example using priorities. The Q-value we leran will be biased according to these priority values which we only wanted to use for sampling. To correct for this bias, we need to introduce an <strong>importance-sampling weight</strong>. We can add another hyperparameter <span class="mathquill ud-math">b</span> and raise each importance-sampling weight to <span class="mathquill ud-math">b</span> to control how much these weights affect learning. In fact, these weights are more improtant towards the end of learning when our Q-values begin to converge. So, we can increase <span class="mathquill ud-math">b</span> from a low value to 1 over time.</p>
            <!-- <div class="row">
               <div class="column">
                  <img src="img/dql43.png" alt="dql43" style="width:100%">
               </div>
            </div> -->
            <img src="img/dql43.png" style="width:60%" alt="" class="img img-fluid" /></br>
            <p>A sequence of convolution layers followed by a couple of fully connected layers that produce Q-values. <U>The core of dueling networks is to use two streams, one that estimates the state value functions and one that estimates the advantage for each action.</U> These streams may share some layers in the beginning such as convolutional layers, then branch off with their own fully connected layers. And, finally the desired Q-values are obtained by combining the state and advantage values.</p>
            <p>The intuition behind this is that the value of most states don't vary a lot across actions. So, it makes sense to try and directly estimate them. But, we still need to captute the difference actions make in each state. This is where the advantage function comes in.</p>
            <p>Some necessary modifications are necessary to adapt Q-Learning to this architecture. Along with double DQNs and priortized replay, this technique has resukted in significant improvements over vanilla DQNs.</p>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Thrun &amp; Schwartz, 1993. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.3097" target="_blank">Issues in Using Function Approximation for Reinforcement Learning</a>. (overestimation of Q-values)</li>
               <li>van Hasselt et al., 2015. <a href="https://arxiv.org/abs/1509.06461" target="_blank">Deep Reinforcement Learning with Double Q-Learning</a>.</li>
               <li>Schaul et al., 2016. <a href="https://arxiv.org/abs/1511.05952" target="_blank">Prioritized Experience Replay</a>.</li>
               <li>Wang et al., 2015. <a href="https://arxiv.org/abs/1511.06581" target="_blank">Dueling Network Architectures for Deep Reinforcement Learning</a>.</li>
               <li>Hausknecht &amp; Stone, 2015. <a href="https://arxiv.org/abs/1507.06527" target="_blank">Deep Recurrent Q-Learning for Partially Observable MDPs</a>.</li>
            </ul>
         </div>
         <footer class="footer">
            <div class="container">
               <div class="row">
                  <div class="col-12">
                     <p class="text-center">
                        Copyright &copy 2024. This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
                     </p>
                  </div>
               </div>
            </div>
         </footer>
      </div>
      <script src="../../assets/js/jquery-3.3.1.min.js"></script>
      <script src="../../assets/js/plyr.polyfilled.min.js"></script>
      <script src="../../assets/js/bootstrap.min.js"></script>
      <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
      <script src="../../assets/js/katex.min.js"></script>
      <script>

         // render math equations
         let elMath = document.getElementsByClassName('mathquill');
         for (let i = 0, len = elMath.length; i < len; i += 1) {
           const el = elMath[i];
         
           katex.render(el.textContent, el, {
             throwOnError: false
           });
         }
         
         // this hack will make sure Bootstrap tabs work when using Handlebars
         if ($('#question-tabs').length && $('#user-answer-tabs').length) {
           $("#question-tabs a.nav-link").on('click', function () {
             $("#question-tab-contents .tab-pane").hide();
             $($(this).attr("href")).show();
           });
           $("#user-answer-tabs a.nav-link").on('click', function () {
             $("#user-answer-tab-contents .tab-pane").hide();
             $($(this).attr("href")).show();
           });
         } else {
           $("a.nav-link").on('click', function () {
             $(".tab-pane").hide();
             $($(this).attr("href")).show();
           });
         }
         
         // side bar events
         $(document).ready(function () {
           $("#sidebar").mCustomScrollbar({
             theme: "minimal"
           });
         
           $('#sidebarCollapse').on('click', function () {
             $('#sidebar, #content').toggleClass('active');
             $('.collapse.in').toggleClass('in');
             $('a[aria-expanded=true]').attr('aria-expanded', 'false');
           });
         });
      </script>
   </body>
</html>