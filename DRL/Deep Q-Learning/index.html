<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta http-equiv="X-UA-Compatible" content="ie=edge">
      <title>Deep Q-Learning</title>
      <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
      <link rel="stylesheet" href="../../assets/css/plyr.css">
      <link rel="stylesheet" href="../../assets/css/katex.min.css">
      <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
      <link rel="stylesheet" href="../../assets/css/styles.css">
      <link rel="shortcut icon" type="image/png" href="../../assets/img/robo-icon.png" />
      <style type="text/css">
         /* Three image containers (use 25% for four, and 50% for two, etc) */
         .column {
         float: left;
         width: 33.33%;
         padding: 5px;
         }
         /* Clear floats after image containers */
         .row::after {
         content: "";
         clear: both;
         display: table;
         }
      </style>
   </head>
   <body>
      <div class="wrapper">
      <nav id="sidebar">
         <div class="sidebar-header">
            <h3>Deep Q-Learning</h3>
         </div>
         <ul class="sidebar-list list-unstyled CTAs">
            <li>
               <a href="../../index.html" class="article">Back to Home</a>
            </li>
         </ul>
         <ul class="sidebar-list list-unstyled components">
            <li class="">
               <a href="#">01. Intro to Deep Q-Learning</a>
            </li>
            <li class="">
               <a href="#">02. Neural Nets as Value Functions</a>
            </li>
            <li class="">
               <a href="#">03. Monte Carlo Learning</a>
            </li>
            <li class="">
               <a href="#">04. Temporal Difference Learning</a>
            </li>
            <li class="">
               <a href="#">05. Q-Learning</a>
            </li>
            <li class="">
               <a href="#">06. Deep Q Network</a>
            </li>
            <li class="">
               <a href="#">07. Experience Replay</a>
            </li>
            <li class="">
               <a href="#">08. Fixed Q Targets</a>
            </li>
            <li class="">
               <a href="#">09. Deep Q-Learning Algorithm</a>
            </li>
            <li class="">
               <a href="#">10. DQN Improvements</a>
            </li>
         </ul>
         <ul class="sidebar-list list-unstyled CTAs">
            <li>
               <a href="../../index.html" class="article">Back to Home</a>
            </li>
         </ul>
      </nav>
      <div id="content">
         <header class="container-fluild header">
            <div class="container">
               <div class="row">
                  <div class="col-12">
                     <div class="align-items-middle">
                        <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                           <div></div>
                           <div></div>
                           <div></div>
                        </button>
                        <h1 style="display: inline-block">Deep Q-Learning</h1>
                     </div>
                  </div>
               </div>
            </div>
         </header>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-td0">Intro to Deep Q-Learning</h2>
            </div>
            <!-- <div class="row">
               <div class="column">
                 <img src="img/dql1.png" alt="dql1" style="width:100%">
               </div>
               </div> -->
            <p>Deep Q-Learning is an algorithm that demonstrates how we can use neural networks to solve RL problems. It has expanded the range of domain we can tackle especially those with large and continuous state spaces.</p>
            <ul>
               <li>How can neural networks be used to represent value functions?</li>
               <li>We will adapt the two main classes of model-free approaches (MC and TD Learning) to work with this new representation.</li>
               <li>Deep Q-Learning (Remember Q-Learning is a variant of TD Learning)</li>
               <li>How can we harness the power of deep learning, including CNNs and RNNs, to teach computers tasks that seem almost impossible to learn from scratch?</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Neural Nets as Value Functions</h2>
               <p>Neural networks (NNs) are considered universal function approximators, capable of approximating any continuous function to a desired degree of accuracy.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql2.png" alt="dql2" style="width:100%">
               </div>
            </div>
            <p>What if we use neural networks to represent our value function?</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql3.png" alt="dql3" style="width:100%">
               </div>
            </div>
            <p>A state value function maps any state to a real number which indicates how valuable that state is according to the current policy <span class="mathquill ud-math">\pi</span>. If we a neural network to approximate this function then the input would need to be fed in as a vector. We already know how to do this using a feature transformation X(s). Now the input can progress through the network and if it is designed to ouput a single real number, that's our value as estimated by the network. This seems to fall in place with the general idea of approximating a value function with the weights of the neural network forming the parameter vector w. The question is <I>how do we learn these parameters?</I></p>
            <div class="row">
               <div class="column">
                  <img src="img/dql4.png" alt="dql4" style="width:100%">
               </div>
            </div>
            <p>If we have a reference or target we are trying to reach, for example <span class="mathquill ud-math">v_{\pi}(s)</span>, then we can use the square difference between the estimated and target value as our error or loss. Then we can backpropagate it through the network adjusting weights along the way to minimize loss. One popular method for adjusting weights is <strong>gradient descent</strong>, where we iteratively change weights a small step away from the direction of error. In order to apply the gradient descent, we need to know the derivative of the value function represented by the network with respect to its weights. This can become very complex, especially for networks with deep architectures but we have pretty efficient algorithms implemented in libraries like tensorflow, pytorch, theano and mxnet to train neural nets for us. All we need is to figure out the loss. That's where our knowledge of RL comes in.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql5.png" alt="dql5" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql6.png" alt="dql6" style="width:100%">
               </div>
            </div>
            <p>At this point, let's also consider the action value function <span class="mathquill ud-math">q_{\pi}(s, a)</span>. The update rule looks very similar to what we had for state value function <span class="mathquill ud-math">v_{\pi}(s)</span>. But we have the same problem here. In most practical problems, there is oracle to tell us what's the correct value function <span class="mathquill ud-math">v_{\pi}(s)</span> or <span class="mathquill ud-math">q_{\pi}(s, a)</span> should be. We need to use a more realistic target, one that is based on our interaction with the environement. This is where RL fundamentally differs from a supervised learning.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Monte Carlo Learning</h2>
               <p>Recall the incremental step that is used in the classical MC learning to update value functions.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql7.png" alt="dql7" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql8.png" alt="dql8" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql9.png" alt="dql9" style="width:100%">
               </div>
            </div>
            <p>Here <span class="mathquill ud-math">G_{t}</span> is the return, i.e., the cummulative discounted reward received following time t. That's a suitable target to attain. So, let's take our neural network update rule and simply substitute the unknown true value function with this return. This gives a concrete update rule for state value functions represented by neural nets or other function approximators. We can do the same for action value functions as well.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql10.png" alt="dql10" style="width:100%">
               </div>
            </div>
            <p>Now that we have the main building block, the update rule, let's build a complete MC algorithm around that. Let's focus on the control problem and adapt our classical MC algorithm to work with a function approximator. It generally includes an evaluation step where we try to estimate the value of each (state, action) pair under the current policy. We do this by interacting with the environment to generate an episode using the policy <span class="mathquill ud-math">\pi</span> and then for each time step t in the episode we update the parameter vector w using the (state, action) pair <span class="mathquill ud-math">(S_{t}, A_{t})</span> and the return <span class="mathquill ud-math">G_{t}</span> computed from the remainder of the episode. This is followed by an improvement step where we extract an ε-greedy policy based on these Q-values. At the beginning we need to initialize our parameter w with random values and start with a policy <span class="mathquill ud-math">\pi</span> defined in the same ε-greedy manner. Then we can repeat these two steps over and over till the weights converge, resulting in the optimal value function and hence the corresponding policy. Note that this is the every visit version of MC. For the first visit version, we only perform the weight update when we see the (state, action) pair for the first time in an episode. </p>
            <p>MC is guaranteed to converge on a local optimum in general; in case of a linear function approximation, it will converge on the global optimum.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Temporal Difference Learning</h2>
               <p>Our second strategy will be to develop a TD technique with function approximation.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql11.png" alt="dql11" style="width:100%">
               </div>
            </div>
            <p>Compare the incremental update step between MC learning where we used the actual return obtained through the episode with the TD case where we use an estimated return. This is called the TD target and in the simplest case TD(0) we use the next reward plus the discounted value of the next state.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql12.png" alt="dql12" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql13.png" alt="dql13" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql14.png" alt="dql14" style="width:100%">
               </div>
            </div>
            <p>In a similar manner as before, we can use this TD target in place of our unknown true value function. This gives us something concrete to work with. Note that we had to adapt the value function to use our function approximator <span class="mathquill ud-math">\hat{V}</span>. This entire difference is called the TD Error and denoted by <span class="mathquill ud-math">δ_{t}</span>. We can extend the same idea to the action value functions as well. Now we are ready to build an algorithm around this update rule!</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql15.png" alt="dql15" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql16.png" alt="dql16" style="width:100%">
               </div>
            </div>
            <p>We will use a TD(0) target and focus on the control problem again. This is esentially the SARSA algorithm. We will begin by initializing parameter w arbitrarily. Our policy <span class="mathquill ud-math">\pi</span> is defined as an ε-greedy choice over our approximate action value function <span class="mathquill ud-math">\hat{q}</span>. Then we start interacting with the environment. For each episode we begin with some initial state S obtained from the environment. We continue interacting till we reach a terminal state. At each time step, we choose an action A to perform and observe the reward R and next state S'. Now we choose another action A' (this time from state S') according to our ε-greedy policy <span class="mathquill ud-math">\pi</span>. This gives us all we need for the SARSA update, (S, A, R, S', A'). We plug these into our gradient descent update rule and adjust our weights accordingly. Finally we simply roll over S' to be the new S, A' to be the new A and repeat. This formulation is useful for episodic tasks where each episode is guaranteed to terminate. It can be adapted to continuing tasks by eliminating the distinct boundary between episodes and treating the sequence of interactions as one long unending episode.</p>
            <p><strong>SARSA is an on-policy algorithm</strong> which means that we are updating the same policy that we are following to carry out actions. This usually works very well and converges pretty quickly because we are using the most updated policy to take each decision. But, this also has some drawbacks! Mainly that the policy being learned and one being followed are intimately tied to each other. What if we wanted to follow one policy, say one that is more exploratory while learning the more optimal policy? That's where we need off-policy algorithms.</p>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Q-Learning</h2>
               <p>Q-Learning is an off-policy variant of TD Learning.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql17.png" alt="dql17" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql18.png" alt="dql18" style="width:100%">
               </div>
            </div>
            <p>Just like SARSA, we initialize w arbitatrily and define an ε-greedy policy <span class="mathquill ud-math">\pi</span> based on the Q-values. Over multiple episodes we use this ε-greedy policy to repeatedly take an action and observe the reward and the next state. The main difference is in the update step. Instead of picking the next action from the same ε-greedy policy, we choose an action greedily which would maximize the expected value going forward. Note that we don't actually take this action, it is only used for performing the update. In fact, we don't even need to pick this action, we can simply use the max Q-value for the next state. This is why, Q-Learning is considered as an off-policy method. We use one policy to take actions (ε-greedy policy <span class="mathquill ud-math">\pi</span>) and yet another policy to perform value updates (a greedy policy). Although both of them are defined on same underlying Q-values, these two are indeed different policies. This is how the Q-Learning looks like for an episodic task. We can use the same algorithm for continuing tasks by treating the whole unending sequence as one long episode or modify it slightly to remove the concept of episodes. Both forms are equivalent. In either case, we may need some additional criteria to figure out when we have fully learned the task or to detect if we are failing measurably. </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql19.png" alt="dql19" style="width:100%">
               </div>
            </div>
            <p>SARSA is an on-policy algorithm that follows the same policy it is learning. In general, it is more suitable for online learning as at any point of time we are using the most updated policy as per our interactions with with the environment. However, it also means that if we use ε-greedy action selection which is necessary to encourage exploration then this randomness also affects the Q-values that are learned. </p>
            <p>On the other hand, Q-Learning is an off-policy method meaning the policy it follows to choose an action is different from the policy it's learning. This can lead to bad online performance because there is a disconnect between the policy we are learning and the one we are following. But the good thing is that the ε-greedy nature of action selection doesn't impact the Q-values learnt. </p>
            <p>Thus, the two appraches have their pros and cons and which algorithm we choose will depend on the characteristics of the environement and our preferences regarding online performance versus more accurate learning.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql20.png" alt="dql20" style="width:100%">
               </div>
            </div>
            <p>The biggest reason off-policy methods like Q-Learning have got so much attention is that they decouple the actions an agent takes in the environment from its learning process. That gives us the opportunity to build various different variations of our learning algorithm. For instance, we can use a more exploratory policy while acting and yet learned the optimal value function. Yes, the online performance will be bad but at some point we can stop exploring and follow the optimal policy for better results. In fact, the policy used to take actions need not be the agent's own. A human can demonstrate what actions to take and the agent can learn from observing the effects of those actions. It also makes it easier to learn offline or in batches since an update to the polcy need not be performed at every step. This as we will see is critical for a lively training neural networks for RL.  </p>
            <p>One drawback of both SARSA & Q-Learning, since they are TD approaches, is that they may not converge on the global optimum when using non-linear function approximation.
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Deep Q Network</h2>
               <p>True to its name, at the heart of the agent is a deep neural network that acts as a function approximator.</p>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql21.png" alt="dql21" style="width:100%">
               </div>
            </div>
            <p>We pass images from our favourite video game one screen at a time and it produces a vector of action values, with the max value indicating the action to take. As a reinforcement signal it is fed back the change in game score at each time step. In the beginning when the neural network is initialized with random values, the actions taken are all over the place. It's really bad as we would expect. But over time, it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well. </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql22.png" alt="dql22" style="width:100%">
               </div>
            </div>
            <p>Consider how complex the input space is, Atari game is displayed at a resolution of 210 X 160 pixels with 128 possible colors for each pixel. This is still technically a discrete state space but very large to process as is. To reduce this complexity, the Deepmind team decided to perform some minimal processing like convert the frames to grayscale and scale them down to a square 84 X 84 pixel block. Square images allowed them to use more optimized neural network operations on GPUs. In order to give the agent access to a sequence of frames they stacked four such frames together resulting in a finite state space size of 84 X 84 X 4. There might be other approaches to dealing with sequential data but this was a simple approach that seem to work pretty well.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql23.png" alt="dql23" style="width:100%">
               </div>
            </div>
            <p>On the output side, unlike a traditional RL setup where only one Q-value is produced at a time, the Deep Q-Network is designed to produce a Q-value for every possible action in a single forward pass. Without this, we would have to run the network individually for every action. Instead we can simply use this vector to take an action, either stochastically or by choosing the one with the maximum value.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql24.png" alt="dql24" style="width:100%">
               </div>
            </div>
            <p>These innovative input and output transformations support a powerful yet simple neural network architecture under the hood. The screen images are first processed by convolutional layers. This allows the system to exploit the spatial relationships and can exploit spatial space. Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames. The original DQN agent uses three such convolutional layers with relu activation. They were followed by one fully-connected hidden layer with relu action and one fully-connected linear output layer that produce the vector of action values. This same architecture was used for all the atari games they tested on but each game was learned from scratch with a freshly initialized network.</p>
            <p>Training such networks require a lot of data but even then it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge due to the high correlation between between actions and states. This can result in a very unstable and ineffective policy. In order to overcome these challenges, the researchers came up with several techniques that slightly modified the base Q-Learning algorithm. We will take a look at two of these techniques.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql25.png" alt="dql25" style="width:100%">
               </div>
            </div>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Mnih et al., 2015. <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank">Human-level control through deep reinforcement learning</a>.</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <h3></h3>
            <div>
               <h2 id="-td-prediction-action-values">Experience Replay</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql26.png" alt="dql26" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql27.png" alt="dql27" style="width:100%">
               </div>
            </div>
            <p>The idea of experience replay and its application of training neural networks for RL isn't new. It was originally proposed to make more efficient use of observed experiences. Consider the basic online Q-Learning algorithm where we interact with the envieonment and at each time step, we obtain a (state, action, reward, next state) tuple, learn from it and then discard it, moving on to the next tuple in the following time step. This seems a little wasteful. We could possibly learn more from these experience tuples if we store them somewhere. Moreover some states are pretty rare to come by and some actions can be pretty costly. So, it would be nice to recall such experiences. That is exactly what a <strong>replay buffer</strong> allows us to do.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql28.png" alt="dql28" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql29.png" alt="dql29" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30.png" alt="dql30" style="width:100%">
               </div>
            </div>
            <p>We store each experience tuple in this buffer as we are interacting with the environment and then sample a small batch of tuples from it in order to learn. As a result, we are able to learn from individual tuples multiple times, recall rare occurences and in general, make better use of our experience. But there is an another critical problem that the experience replay can help with and this is what DQN takes advantage of. If we think about the expeiences being obtained, we will realize that every action <span class="mathquill ud-math">A_{t}</span> affects the next state <span class="mathquill ud-math">S_{t}</span> in some way which means that a sequence of expeience tuples can be highly correlated. A naive Q-Learning approach that learns from each of these expeiences in sequantial order runs the risk of getting swayed by the effect of this correlation. With experience replay we can sample from this buffer at random. It doesn't have to in the same sequence as we stored the tuples. This helps break the correlation and ultimately prevents action values from oscillating or diverging catastrophically.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql30-1.png" alt="dql30-1" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30-2.png" alt="dql30-2" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql30-3.png" alt="dql30-3" style="width:100%">
               </div>
            </div>
            <p>Let's look at an example. I am learning to play tennis and I like to practice rallying against the wall. I am more confident with my forehand shot than my backhand shot and I can hit the ball fairly straight. So the ball keeps coming back around the same area to my right and I keep hitting forehand shots. If I were an online Q-Learning agent, learning as I play, this is what I might pick up. When the ball comes to my right, I should hit with my forehand less certainly at first but with increase in confidence as I repeatedly hit the ball. Great! I am learning to play forehand pretty well. But I am not exploring rest of the state space. This could be addressed by using an ε-greedy policy acting randomly with a small chance. So I try different combinations of states and actions and sometimes I make mistakes but I eventually figure out the best overall policy. Use a forehand shot when the ball comes to my right and backhand when it comes to my left. This learning strategy seems to work with a Q table where we have assumed this highly simplified state space with just two discrete states. But when we consider a continuous state space, things can fall apart. First, the ball can come anywhere between the extreme left and extreme right. If I try to discretized this range into small buckets, I will have too many possibilities. What if I endup learning a policy with too may holes in it, states or situations that I maynot have visited during practice. In fact, it makes more sense to use a function approximator like a linear combination of RBF kernels or a Q network that can generalize my learning across the space. Now everytime the ball comes to my right and I successfully hit a forehand shot, my value function changes slightly. It becomes more positive around the exact region where the ball came but also raises the value of forehand shot, in general, across the state space. The effect is less pronounce away from the exact spot but over time it can add up. And that's exactly what happens when I tried to learn while playing processing each experience tuple in order. For instance, if my forehand shot is farely straight, I likely get back the ball around the same spot. This produces a state pretty similar to the previous one, so I use my forehand again and if it is successful, it reinforces my belief that forehand is a good choice. I can easily get trapped in the cycle. Ultimately, if I don't see too many examples of ball coming to my left for a while, then the value of forehand shot can become greater than back hand across the entire state space. My policy would be then to choose forehand regardless of where I see the ball coming. Disaster! <em>How can we fix this?</em> </p>
            <div class="row">
               <div class="column">
                  <img src="img/dql30-4.png" alt="dql30-4" style="width:100%">
               </div>
            </div>
            <p>The first thing I should do is to stop learning while practicing. This time is best spend in trying out different shots playing a little randomly and thus exploring the state space. It then becomes important to remember my interactions, what shots worked well in what situation, etc. When I take a break or when I am back home and resting, that's a good time to recall these experiences and learn from them. The main advantage is that now I have a more comprehensive set of examples, somewhere the ball came to my right, somewhere it cam to my left, some forehand shots and some backhand.  I can generalize patterns from across these examples, recalling them in whatever order I please. This helps me avoid being fixated on one region of the state space or reinforcing the same action over and over. After a round of learning, I can go back to playing with my updated value function, again collect a bunch of experiences and then learn from them in a batch. In this way, experience replay can help us learn a more robust policy, one that is not affected by the inherent correlation present in the sequence of observed experience tuples.</p>
            <div class="row">
               <div class="column">
                  <img src="img/dql31.png" alt="dql31" style="width:100%">
               </div>
            </div>
            <p>If we think about it, this approach is basically building a database of samples and then learning a mapping from them. In that sense, expeience replay helps us to reduce the RL problem or atleast the value learning portion of it into a supervised learning scenario. We can then apply other models, learning techniques and best practices developed in the supervised learning literature to RL. We can even improve upon this idea, for example by prioritizing experience tuples that are rare or more important. </p>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Long-Ji Lin, 1993. <a href="https://pdfs.semanticscholar.org/54c4/cf3a8168c1b70f91cf78a3dc98b671935492.pdf" target="_blank">Reinforcement learning for robots using neural networks</a>.</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Fixed Q Targets</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql32.png" alt="dql32" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql33.png" alt="dql33" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql34.png" alt="dql34" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql35.png" alt="dql35" style="width:100%">
               </div>
            </div>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">Deep Q-Learning Algorithm</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql36.png" alt="dql36" style="width:100%">
               </div>
            </div>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Mnih et al., 2015. <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank">Human-level control through deep reinforcement learning</a>. (DQN paper)</li>
               <li>He et al., 2015. <a href="https://arxiv.org/abs/1502.01852" target="_blank">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>. (weight initialization)</li>
            </ul>
         </div>
         <div class="divider"></div>
         <div class="ud-atom">
            <div>
               <h2 id="-td-prediction-action-values">DQN Improvements</h2>
            </div>
            <div class="row">
               <div class="column">
                  <img src="img/dql37.png" alt="dql37" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql38.png" alt="dql38" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql39.png" alt="dql39" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql40.png" alt="dql40" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql41.png" alt="dql41" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql42.png" alt="dql42" style="width:100%">
               </div>
               <div class="column">
                  <img src="img/dql43.png" alt="dql43" style="width:100%">
               </div>
            </div>
            <p><strong>Readings</strong></p>
            <ul>
               <li>Thrun &amp; Schwartz, 1993. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.3097" target="_blank">Issues in Using Function Approximation for Reinforcement Learning</a>. (overestimation of Q-values)</li>
               <li>van Hasselt et al., 2015. <a href="https://arxiv.org/abs/1509.06461" target="_blank">Deep Reinforcement Learning with Double Q-Learning</a>.</li>
               <li>Schaul et al., 2016. <a href="https://arxiv.org/abs/1511.05952" target="_blank">Prioritized Experience Replay</a>.</li>
               <li>Wang et al., 2015. <a href="https://arxiv.org/abs/1511.06581" target="_blank">Dueling Network Architectures for Deep Reinforcement Learning</a>.</li>
               <li>Hausknecht &amp; Stone, 2015. <a href="https://arxiv.org/abs/1507.06527" target="_blank">Deep Recurrent Q-Learning for Partially Observable MDPs</a>.</li>
            </ul>
         </div>

         <footer class="footer">
           <div class="container">
              <div class="row">
                 <div class="col-12">
                    <p class="text-center">
                       <a href="#" target="_blank">&copy 2024 <strong>Anubhav Singh</strong></a>
                    </p>
                 </div>
              </div>
           </div>
        </footer>
      </div>
      
      <script src="../../assets/js/jquery-3.3.1.min.js"></script>
      <script src="../../assets/js/plyr.polyfilled.min.js"></script>
      <script src="../../assets/js/bootstrap.min.js"></script>
      <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
      <script src="../../assets/js/katex.min.js"></script>
      <script>
         // Initialize Plyr video players
         const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));
         
         // render math equations
         let elMath = document.getElementsByClassName('mathquill');
         for (let i = 0, len = elMath.length; i < len; i += 1) {
           const el = elMath[i];
         
           katex.render(el.textContent, el, {
             throwOnError: false
           });
         }
         
         // this hack will make sure Bootstrap tabs work when using Handlebars
         if ($('#question-tabs').length && $('#user-answer-tabs').length) {
           $("#question-tabs a.nav-link").on('click', function () {
             $("#question-tab-contents .tab-pane").hide();
             $($(this).attr("href")).show();
           });
           $("#user-answer-tabs a.nav-link").on('click', function () {
             $("#user-answer-tab-contents .tab-pane").hide();
             $($(this).attr("href")).show();
           });
         } else {
           $("a.nav-link").on('click', function () {
             $(".tab-pane").hide();
             $($(this).attr("href")).show();
           });
         }
         
         // side bar events
         $(document).ready(function () {
           $("#sidebar").mCustomScrollbar({
             theme: "minimal"
           });
         
           $('#sidebarCollapse').on('click', function () {
             $('#sidebar, #content').toggleClass('active');
             $('.collapse.in').toggleClass('in');
             $('a[aria-expanded=true]').attr('aria-expanded', 'false');
           });
         });
      </script>
   </body>
</html>