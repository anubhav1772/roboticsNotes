<!-- udacimak v1.2.2 -->
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Summary</title>
  <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../../assets/css/plyr.css">
  <link rel="stylesheet" href="../../assets/css/katex.min.css">
  <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../../assets/css/styles.css">
  <link rel="shortcut icon" type="image/png" href="../../assets/img/udacimak.png" />
  <style type="text/css">
    /* Three image containers (use 25% for four, and 50% for two, etc) */
.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

  </style>
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>The RL Framework: The Solution</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="#">01. Introduction</a>
    </li>
    <li class="">
      <a href="#">02. Policies</a>
    </li>
    <li class="">
      <a href="#">03. State-Value Functions</a>
    </li>
    <li class="">
      <a href="#">04. Bellman Equations</a>
    </li>
    <li class="">
      <a href="#">05. Optimality</a>
    </li>
    <li class="">
      <a href="#">06. Action-Value Functions</a>
    </li>
    <li class="">
      <a href="#">07. Optimal Policies</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">The RL Framework: The Solution</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <div>
  <h5 id="summary">In reinforcement learning, agents learn to prioritize different decisions based on the rewards and punishments associated with different outcomes.</h5>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-09-25-at-11.35.38-am.png" alt="State-value function for golf-playing agent (Sutton and Barto, 2017)" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>State-value function for golf-playing agent (Sutton and Barto, 2017)</p>
    </figcaption>
  </figure>
</div>


</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-policies">Policies</h2>
  <div class="row">
    <div class="column">
      <img src="img/rlfs1.png" alt="rlfs1" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs2.png" alt="rlfs2" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs3.png" alt="rlfs3" style="width:100%">
    </div>
  </div>
<ul>
<li>A policy determines how an agent chooses an action in response to the current state.  In other words, it specifies how the agent responds to situations that the environment has presented.</li>
<li>A <strong>deterministic policy</strong> is a mapping <span class="mathquill ud-math">\pi: \mathcal{S}\to\mathcal{A}</span>.  For each state <span class="mathquill ud-math">s\in\mathcal{S}</span>, it yields the action <span class="mathquill ud-math">a\in\mathcal{A}</span> that the agent will choose while in state <span class="mathquill ud-math">s</span>.</li>
<li>A <strong>stochastic policy</strong> is a mapping <span class="mathquill ud-math">\pi: \mathcal{S}\times\mathcal{A}\to [0,1]</span>.  For each state <span class="mathquill ud-math">s\in\mathcal{S}</span> and action <span class="mathquill ud-math">a\in\mathcal{A}</span>, it yields the probability <span class="mathquill ud-math">\pi(a|s)</span> that the agent chooses action <span class="mathquill ud-math">a</span> while in state  <span class="mathquill ud-math">s</span>.</li>
</ul>
</div>
Consider the example of recycling robot MDP.
<p><h5 id="-deterministic-policy-example">Deterministic Policy </h5>An example deterministic policy <span class="mathquill ud-math">\pi: \mathcal{S}\to\mathcal{A}</span> can be specified as:</p>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{low}) = \text{recharge}</span></p>
</blockquote>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{high}) = \text{search}</span></p>
</blockquote>
<p>In this case,</p>
<ul>
<li>if the battery level is <em>low</em>, the agent chooses to <em>recharge</em> the battery.  </li>
<li>if the battery level is <em>high</em>, the agent chooses to <em>search</em> for cans. </li>
</ul>

<p><h5 id="-stochastic-policy-example">Stochastic Policy </h5>An example stochastic policy <span class="mathquill ud-math">\pi: \mathcal{S}\times\mathcal{A}\to [0,1]</span> can be specified as:</p>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{recharge}|\text{low}) = 0.5</span></p>
</blockquote>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{wait}|\text{low}) = 0.4</span></p>
</blockquote>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{search}|\text{low}) = 0.1</span></p>
</blockquote>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{search}|\text{high}) = 0.9</span></p>
</blockquote>
<blockquote>
  <p><span class="mathquill ud-math">\pi(\text{wait}|\text{high}) = 0.1</span></p>
</blockquote>
<p>In this case,</p>
<ul>
<li>if the battery level is <em>low</em>, the agent <em>recharges</em> the battery with 50% probability, <em>waits</em> for cans with 40% probability, and <em>searches</em> for cans with 10% probability.  </li>
<li>if the battery level is <em>high</em>, the agent <em>searches</em> for cans with 90% probability and <em>waits</em> for cans with 10% probability. </li>
</ul>
</div>


<div class="divider"></div>
<div class="ud-atom">
  <h3></h3>
<div>
<h2 id="-state-value-functions">State-Value Functions</h2>

<!-- <div class="divider"></div> -->
<div class="row">
    <div class="column">
      <img src="img/rlfs4.png" alt="rlfs4" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs5.png" alt="rlfs5" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs6.png" alt="rlfs6" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs7.png" alt="rlfs7" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs8.png" alt="rlfs8" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs9.png" alt="rlfs9" style="width:100%">
    </div>
    <div class="column">
      <img src="img/statevaluefun.png" alt="statevaluefun" style="width:100%">
    </div>
    <div class="column">
      <img src="img/rlfs10.png" alt="rlfs10" style="width:100%">
    </div>
  </div>

<ul>
<li>The <strong>state-value function</strong> for a policy <span class="mathquill ud-math">\pi</span> is denoted <span class="mathquill ud-math">v_\pi</span>.  For each state <span class="mathquill ud-math">s \in\mathcal{S}</span>, it yields the expected return if the agent starts in state <span class="mathquill ud-math">s</span> and then uses the policy to choose its actions for all time steps.  That is, <span class="mathquill ud-math">v_\pi(s) \doteq \text{} \mathbb{E}\pi[G_t|S_t=s]</span>.  We refer to <span class="mathquill ud-math">v_\pi(s)</span> as the <strong>value of state <span class="mathquill ud-math">s</span> under policy <span class="mathquill ud-math">\pi</span></strong>.</li>
  <ul><li>The state-value function will always correspond to a particular policy. So, if we change a policy, we change the state-value function.</li></ul>
<li>The notation <span class="mathquill ud-math">\mathbb{E}\pi[\cdot]</span> is defined as the expected value of a random variable, given that the agent follows policy <span class="mathquill ud-math">\pi</span>.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-bellman-equations">Bellman Equations</h2>

<div class="row">
  <div class="column">
    <img src="img/rlfs11.png" alt="rlfs11" style="width:100%">
  </div>
  <div class="column">
    <img src="img/rlfs12.png" alt="rlfs12" style="width:100%">
  </div>
</div>
<ul>
<li>The <strong>Bellman expectation equation for <span class="mathquill ud-math">v_\pi</span></strong> is: <span class="mathquill ud-math">v_\pi(s) = \text{} \mathbb{E}\pi[R{t+1} + \gamma v_\pi(S_{t+1})|S_t =s].</span></li>
</ul>

<!-- <div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div> -->
  <p>In the gridworld example, once the agent selects an action, </p>
<ul>
<li>it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and </li>
<li>the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution).</li>
</ul>
<p>In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state.  </p>
<p>For a general MDP, we have to instead work in terms of an <em>expectation</em>, since it's not often the case that the immediate reward and next state can be predicted with certainty.  Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP.  In this case, where the reward <span class="mathquill ud-math">r</span> and next state <span class="mathquill ud-math">s'</span> are drawn from a (conditional) probability distribution <span class="mathquill ud-math">p(s',r|s,a)</span>, the <strong>Bellman Expectation Equation (for <span class="mathquill ud-math">v_\pi</span>)</strong> expresses the value of any state <span class="mathquill ud-math">s</span> in terms of the <em>expected</em> immediate reward and the  <em>expected</em> value of the next state:</p>
<div class="mathquill">v_\pi(s) = \text{} \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t =s].</div>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-calculating-the-expectation">Calculating the Expectation</h2>
<p>In the event that the agent's policy <span class="mathquill ud-math">\pi</span> is <strong>deterministic</strong>,  the agent selects action <span class="mathquill ud-math">\pi(s)</span> when in state <span class="mathquill ud-math">s</span>, and the Bellman Expectation Equation can be rewritten as the sum over two variables (<span class="mathquill ud-math">s'</span> and <span class="mathquill ud-math">r</span>):</p>
<div class="mathquill">v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R}}p(s',r|s,\pi(s))(r+\gamma  v_\pi(s'))</div>
<p>In this case, we multiply the sum of the reward and discounted value of the next state <span class="mathquill ud-math">(r+\gamma  v_\pi(s'))</span> by its corresponding probability <span class="mathquill ud-math">p(s',r|s,\pi(s))</span> and sum over all possibilities to yield the expected value.</p>
<p>If the agent's policy <span class="mathquill ud-math">\pi</span> is <strong>stochastic</strong>,  the agent selects action <span class="mathquill ud-math">a</span> with probability <span class="mathquill ud-math">\pi(a|s)</span> when in state <span class="mathquill ud-math">s</span>, and the Bellman Expectation Equation can be rewritten as the sum over three variables (<span class="mathquill ud-math">s'</span>, <span class="mathquill ud-math">r</span>, and <span class="mathquill ud-math">a</span>):</p>
<div class="mathquill">v_\pi(s) = \text{} \sum_{s'\in\mathcal{S}^+, r\in\mathcal{R},a\in\mathcal{A}(s)}\pi(a|s)p(s',r|s,a)(r+\gamma  v_\pi(s'))</div>
<p>In this case, we multiply the sum of the reward and discounted value of the next state <span class="mathquill ud-math">(r+\gamma  v_\pi(s'))</span> by its corresponding probability <span class="mathquill ud-math">\pi(a|s)p(s',r|s,a)</span> and sum over all possibilities to yield the expected value.</p>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-there-are-3-more-bellman-equations">There are 3 more Bellman Equations!</h2>
<p>Here we learned about one Bellman equation, but there are 3 more, for a total of 4 Bellman equations.</p>
<blockquote>
  <p>All of the Bellman equations attest to the fact that value functions satisfy recursive relationships.  </p>
</blockquote>
<p>For instance, the  <strong>Bellman Expectation Equation (for <span class="mathquill ud-math">v_\pi</span>)</strong> shows that it is possible to relate the value of a state to the values of all of its possible successor states.</p>
</div>

</div>


<!-- </div> -->

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-optimality">Optimality</h2>
<div class="ud-atom">
  <div class="row">
  <div class="column">
    <img src="img/optimality.png" alt="optimality" style="width:100%">
  </div>
  <div class="column">
    <img src="img/optimal_policy.png" alt="optimal_policy" style="width:100%">
  </div>
  <div class="column">
    <img src="img/rlfs13.png" alt="rlfs13" style="width:100%">
  </div>
  <div class="column">
    <img src="img/multi_optimal_policies.png" alt="multi_optimal_policies" style="width:100%">
  </div>
</div>
<!--   <div>
  <figure class="figure">
    <img src="img/optimality.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/optimal_policy.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/multi_optimal_policies.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
</div> -->
<ul>
<li>A policy <span class="mathquill ud-math">\pi'</span> is defined to be better than or equal to a policy <span class="mathquill ud-math">\pi</span> if and only if <span class="mathquill ud-math">v_{\pi'}(s) \geq v_\pi(s)</span> (state value function of policy <span class="mathquill ud-math">\pi'</span> is greater than or equal to state value function of policy <span class="mathquill ud-math">\pi</span>) for all <span class="mathquill ud-math">s\in\mathcal{S}</span>.</li>
(NOTE: It is often possible to find two policies that cannot be compared.)
<li>An <strong>optimal policy <span class="mathquill ud-math">\pi_*</span></strong> satisfies <span class="mathquill ud-math">\pi_* \geq \pi</span> for all policies <span class="mathquill ud-math">\pi</span>.  An optimal policy is guaranteed to exist but may not be unique.</li>
<li>All optimal policies have the same state-value function <span class="mathquill ud-math">v_*</span>, called the <strong>optimal state-value function</strong>.</li>
</ul>
</div>

</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-action-value-functions">Action-Value Functions</h2>
<!--   <div>
  <figure class="figure">
    <img src="img/stateval_actionval.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/state-action-val.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/action-val-fun.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/state-action.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
</div> -->

<div class="row">
  <div class="column">
    <img src="img/stateval_actionval.png" alt="stateval_actionval" style="width:100%">
  </div>
  <div class="column">
    <img src="img/state-action-val.png" alt="state-action-val" style="width:100%">
  </div>
  <div class="column">
    <img src="img/rlfs14.png" alt="rlfs14" style="width:100%">
  </div>
  <div class="column">
    <img src="img/rlfs15.png" alt="rlfs15" style="width:100%">
  </div>
  <div class="column">
    <img src="img/action-val-fun.png" alt="action-val-fun" style="width:100%">
  </div>
  <div class="column">
    <img src="img/rlfs16.png" alt="rlfs16" style="width:100%">
  </div>
  <div class="column">
    <img src="img/state-action.png" alt="state-action" style="width:100%">
  </div>
</div>
<ul>
<li>The <strong>action-value function</strong> for a policy <span class="mathquill ud-math">\pi</span> is denoted <span class="mathquill ud-math">q_\pi</span>.  For each state <span class="mathquill ud-math">s \in\mathcal{S}</span> and action <span class="mathquill ud-math">a \in\mathcal{A}</span>, it yields the expected return if the agent starts in state <span class="mathquill ud-math">s</span>, takes action <span class="mathquill ud-math">a</span>, and then follows the policy for all future time steps.  That is, <span class="mathquill ud-math">q_\pi(s,a) \doteq \mathbb{E}\pi[G_t|S_t=s, A_t=a]</span>.  We refer to <span class="mathquill ud-math">q_\pi(s,a)</span> as the <strong>value of taking action <span class="mathquill ud-math">a</span> in state <span class="mathquill ud-math">s</span> under a policy <span class="mathquill ud-math">\pi</span></strong> (or alternatively as the <strong>value of the state-action pair <span class="mathquill ud-math">s, a</span></strong>).</li>
<li>All optimal policies have the same action-value function <span class="mathquill ud-math">q_*</span>, called the <strong>optimal action-value function</strong>.</li>
</ul>
</div>
</div>

<div class="divider"></div><div class="ud-atom">
  <div>
  <h2 id="-optimal-policies">Optimal Policies</h2>
<!--   <div>
<figure class="figure">
    <img src="img/optimal-policy.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
  <figure class="figure">
    <img src="img/one-of-optimal-policy.png" width="50%" height="50%" alt="gridworld" class="img img-fluid">
  </figure>
</div> -->
<div class="row">
  <div class="column">
    <img src="img/optimal-policy.png" alt="optimal-policy" style="width:100%">
  </div>
  <div class="column">
    <img src="img/one-of-optimal-policy.png" alt="one-of-optimal-policy" style="width:100%">
  </div>
</div>

<p>The main idea is that the agent interacts with the environment, and from that interaction it estimate the optimal action-value function. Then, the agent uses that value function to get the optimal policy.</p>
<p>If the state space  <span class="mathquill ud-math">\mathcal{S}</span> and action space <span class="mathquill ud-math">\mathcal{A}</span> are finite, we can represent the optimal action-value function <span class="mathquill ud-math">q_*</span> in a table, where we have one entry for each possible environment state <span class="mathquill ud-math">s \in \mathcal{S}</span> and action <span class="mathquill ud-math">a\in\mathcal{A}</span>. </p>
<p>The value for a particular state-action pair <span class="mathquill ud-math">s,a</span> is the expected return if the agent starts in state <span class="mathquill ud-math">s</span>, takes action <span class="mathquill ud-math">a</span>, and then henceforth follows the optimal policy <span class="mathquill ud-math">\pi_*</span>.  </p>
<p>Once the agent determines the optimal action-value function <span class="mathquill ud-math">q_*</span>, it can quickly obtain an optimal policy <span class="mathquill ud-math">\pi_*</span> by setting  <span class="mathquill ud-math">\pi_(s) = \arg\max_{a\in\mathcal{A}(s)} q_(s,a)</span> for all <span class="mathquill ud-math">s\in\mathcal{S}</span>.</p>
<p>To see why this should be the case, note that it must hold that <span class="mathquill ud-math">v_(s) = \max_{a\in\mathcal{A}(s)} q_(s,a)</span>.</p>
<p>In the event that there is some state <span class="mathquill ud-math">s\in\mathcal{S}</span> for which multiple actions <span class="mathquill ud-math">a\in\mathcal{A}(s)</span> maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions.  You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.</p>

<img src="img/q_star.png" width="50%" height="50%" alt="gridworld" class="img img-fluid"/>
<p>Towards constructing the optimal policy, we can begin by selecting the entries that maximize the action-value function, for each row (or state).</p>

</div>
</div>

</div>
</div>
</main>

<footer class="footer">
  <div class="container">
    <div class="row">
      <div class="col-12">
        <p class="text-center">
          <a href="#" target="_blank">&copy 2024 <strong>Anubhav Singh</strong></a>
        </p>
      </div>
    </div>
  </div>
</footer>

</div>
</div>

  <script src="../../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../../assets/js/bootstrap.min.js"></script>
  <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
