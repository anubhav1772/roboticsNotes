<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>RL in Continuous Spaces</title>
    <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../assets/css/plyr.css">
    <link rel="stylesheet" href="../../assets/css/katex.min.css">
    <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
    <link rel="stylesheet" href="../../assets/css/styles.css">
    <link rel="shortcut icon" type="image/png" href="../../assets/img/robo-icon.png" />
    <style type="text/css">
        /* Three image containers (use 25% for four, and 50% for two, etc) */
    .column {
      float: left;
      width: 33.33%;
      padding: 5px;
    }
    
    /* Clear floats after image containers */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }
    </style>
</head>

<body>
    <div class="wrapper">
        <nav id="sidebar">
            <div class="sidebar-header">
                <h3>RL in Continuous Spaces</h3>
            </div>

            <ul class="sidebar-list list-unstyled CTAs">
                <li>
                    <a href="../../index.html" class="article">Back to Home</a>
                </li>
            </ul>

            <ul class="sidebar-list list-unstyled components">
                <li class="">
                    <a href="#">01. Reinforcement Learning : Review</a>
                </li>
                <li class="">
                    <a href="#">02. Discrete vs. Continuous Spaces</a>
                </li>
                <li class="">
                    <a href="#">03. Discretization</a>
                </li>
                <li class="">
                    <a href="#">04. Tile Coding</a>
                </li>
                <li class="">
                    <a href="#">05. Coarse Coding</a>
                </li>
                <li class="">
                    <a href="#">06. Function Approximation</a>
                </li>
                <li class="">
                    <a href="#">07. Linear Function Approximation</a>
                </li>
                <li class="">
                    <a href="#">08. Kernel Functions</a>
                </li>
                <li class="">
                    <a href="#">09. Non-Linear Function Approximation</a>
                </li>


            </ul>

            <ul class="sidebar-list list-unstyled CTAs">
                <li>
                    <a href="../../index.html" class="article">Back to Home</a>
                </li>
            </ul>
        </nav>

        <div id="content">
            <header class="container-fluild header">
                <div class="container">
                    <div class="row">
                        <div class="col-12">
                            <div class="align-items-middle">
                                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                                    <div></div>
                                    <div></div>
                                    <div></div>
                                </button>

                                <h1 style="display: inline-block">RL in Continuous Spaces</h1>
                            </div>
                        </div>
                    </div>
                </div>
            </header>

            <div class="ud-atom">
                <h3></h3>
                <div>
                    <h2 id="-td-prediction-td0">Reinforcement Learning : Review</h2>
                </div>
                <div class="row">
                    <div class="column">
                        <img src="img/drl1.png" alt="drl1" style="width:100%">
                    </div>
                    <div class="column">
                        <img src="img/drl2.png" alt="drl2" style="width:100%">
                    </div>
                </div>
                <br /> RL problems are typically framed as Markov Decision Processes or MDPs. An MDP consist of set of states <span class="mathquill ud-math">\mathcal{S}</span>, actions <span class="mathquill ud-math">\mathcal{A}</span>, along with probabilities
                <span class="mathquill ud-math">\mathcal{P}</span>, rewards <span class="mathquill ud-math">\mathcal{R}</span> and a discount factor <span class="mathquill ud-math">\mathcal{γ}</span>.
                <br />
                <br />
                <p><span class="mathquill ud-math">\mathcal{P}</span> captures how frequently different transitions and rewards occur, often modeled as a single joint probability where the state and reward at any time step (t+1) depend only on the state
                    and action taken at the previous time step t. This characteristics of certain environments is known as the <strong>Markov Property</strong>.</p>

                <p><span class="mathquill ud-math">\mathcal{R}</span> is the set of all rewards. The reward probability is jointly specified with the transition probability as: <span class="mathquill ud-math">p(s', r | s, a) = \mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t=s, A_t=a)</span></p>

                <p>There are two quantities we are typically interested in:</p>
                <ul>
                    <li>Value of a state, <strong>V(S)</strong> which we try to estimate or predict.</li>
                    <li>Value of an action taken in a certain state, <strong>Q(S, A)</strong> which can help us to decide which action to take.</li>
                </ul>
                These two mappings or functions are very much interrelated and help us find an optimal policy for a problem <span class="mathquill ud-math">\pi_*</span> that maximizes the total reward received.
                <br />
                <br />

                <p>Note that since MDPs are probabilistic in nature, we can't predict with complete certainty what future rewards we will get and for how long. So, we typically aim for the <strong>total expected reward</strong>. This is where the discount
                    factor <span class="mathquill ud-math">\mathcal{γ}</span> comes into play as well. It is used to assign a lower weightage to the future rewards when computing state and action values.</p>

                <p>RL algorithms are generally classified into two groups:</p>

                <ul>
                    <li><strong>Model-Based Learning (Dynamic Programming)</strong></li>
                    <ul>
                        <li>Policy Iteration</li>
                        <li>Value Iteration</li>
                    </ul>
                    Model-Based approaches such as Policy and Value iterations require a known transition and reward model. They essentially apply dynamic programming to iteratively compute the desired value functions and optimal policies using that model.
                    <li><strong>Model-Free Learning</strong></li>
                    <ul>
                        <li>Monte Carlo Methods</li>
                        <li>Temporal-Difference Learning</li>
                    </ul>
                    Model-Free approaches don't require an explicit model. They sample the environment by carrying out exploratory actions and use the experience gained to directly estimate the value functions.
                </ul>

                <p><strong>Deep Reinforcement Learning</strong> refers to approaches that use deep learning, mainly multi-layer neural networks to solve reinforcement learning problems. </p>

                <p>RL is typically characterize by finite MDPs where the number of states and actions are limited. But, there are so many problems where the space of state and action is very large or even made up of continuous real-valued numbers. Traditional
                    algorithms use a table or dictionary or other finite structure to capture state and action values. They no longer work for such problems.</p>

                <!-- <img src="img/drl4.png" alt="drl4" style="width:40%" class="img img-fluid"/><br /> -->
                <ul>
                    <li>RL in Continuous Spaces</li>
                    <li>Deep Q-Learning</li>
                    <li>Policy Gradients</li>
                    <li>Actor-Critic Methods</li>
                </ul>

                <!-- <div class="row">
  <div class="column">
    <div class="column">
    <img src="img/drl4.png" alt="drl4" style="width:100%">
  </div> 
  </div>
</div> -->

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Discrete vs. Continuous Spaces</h2>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl5.png" alt="drl5" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl6.png" alt="drl6" style="width:100%">
                            </div>
                        </div>
                        <p>Recall the definition of a MDP where we assume that the environment state at any time is drawn from a set of possible states. When this set is finite, we can call it a discrete state space. Similarly with actions. If there is finite
                            set of them, the environment is set to have a discrete action space.</p>
                        <p>Having discrete spaces simplies things for us. It allows us to represent any function of states and actions as a dictionary or look up table.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl7.png" alt="drl7" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl8.png" alt="drl8" style="width:100%">
                            </div>
                        </div>
                        <p> Discrete spaces are also critical to a number of RL algorithms, for instance, in value iteration (shown below) the internal for-loop goes over each state s one by one and updates the corresponding value estimate V(s). This is impossible
                            if we have an infinite state space. The loop would go on for ever. Even for a discrete state spaces with a lot of space this can quickly become infeasible. Model-free methods like Q-Learning assumes discrete spaces as well.
                            Here the mex is computed over all possible actions from state S' which is easy when we have a finite set of actions. But, this tiny step itself becomes a full-blown optimization problem if our action space is continuous. </p>
                        <p><b>Why do we exactly mean by Continuous Spaces?</b></p>
                    </div>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl9.png" alt="drl9" style="width:100%">
                        </div>
                        <div class="column">
                            <img src="img/drl10.png" alt="drl10" style="width:100%">
                        </div>
                    </div>
                    <p>A Continuous Space is not restricted to a set of distinct values like integers. Instead it can take a range of values typically real numbers. This means quantities like state values that could be depicted as say a bar chart for a discrete
                        case (one bar for every state) will now need to be though as a density plot over a desired range. The same notion extends to an environment where a state is no longer a single real valued number but a vector of such numbers. This
                        is still referred to as a Continuous Space just with more than one dimensions. </p>
                    <p><b>Why are Continuous state spaces important?</b></p>
                    <p>In general grid-Based worlds are very popular in RL. They give us a glimpse at how agent might act in a spatial environment. But, real physical spaces are not always neatly divided up into grids. There is no cell (5, 3) for the vacuum
                        cleaner robot to go to. It has to chart a course from its current position to say 2.5m from the west wall, by 1.8m from the north wall. It also has to keep track of its heading and turns smoothly to face the direction it wants
                        to move in. These are all real numbers that the agent may need to process and represent as part of the state. </p>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl11.png" alt="drl11" style="width:100%">
                        </div>
                        <div class="column">
                            <p><strong>Actions too can be continuous.</strong> Take for example, a robot that plays darts. It has to set the height and the angle it wants to release the dart at, choose an appropriate level of power with which to throw, etc.
                                Even a small difference in these values can have a large impact on where the dart will ultimately lands on the board.</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="column">
                            <img src="img/drl12.png" alt="drl12" style="width:100%">
                        </div>
                    </div>
                    <p>In general, most actions that need to take place in a physical environment are continuous in nature. </p>
                    <p>Clearly, we need to modify our representation or algorithms or both to accomodate continuous spaces. The two main strategies we will be looking at:</p>
                    <ul>
                        <li>Discretization</li>
                        <li>Function Approximation</li>
                    </ul>


                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Discretization</h2>
                        <ul>
                            <li>Discretization is basically converting a continuous space into a discrete one.</li>
                        </ul>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl13.png" alt="drl13" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl14.png" alt="drl14" style="width:100%">
                            </div>
                        </div>
                        <p>Remember our continuous vacuum cleaner world? All we are saying is let's bring back a grid structure with discrete positions identified. Note that we are not really forcing our agent to be in exactly the center of these positions
                            since the underlying world is continuous we don't have control over that. But in our representation of the state space we only identify certain positions as relevant. For instance, whether the robot is at (3.1, 2.4) or (2.9,
                            1.8), we can round that off to (3, 2). This will almost always be a little incorrect but for some environment discretizing the state space can work out very well. It enables us to use the existing algorithms with little or
                            no modifications. Actions can be discretize as well, for example, angles can be divided into whole degrees or even 90 degrees increments if appropriate.</p>

                        <p>Now imagine there are objects in this discretized world, obstacles that the robot may need to avoid. With our grid representation all we can do is mark off the cells where the object is present even by a little. This is known as
                            an <strong>Occupancy Grid</strong>. But our choice of discretization may lead the agent into thinking there is no path across these obstacles to reach some desired locations. </p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl15.png" alt="drl15" style="width:100%">
                            </div>

                        </div>
                        <p>Instead we could vary the grid according to these obstacles, then we could open up a feasible path for the agent.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl16.png" alt="drl16" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl17.png" alt="drl17" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl18.png" alt="drl18" style="width:100%">
                            </div>
                        </div>
                        <p>An alternate approach would be to divide up the grid into smaller cells where required. It would still be an approximation but it will allow us to allocate more of our state representation to where it matters better than dividing
                            the entire state space into finer cells which may increase the total number of states and in turn the time needed to compute value functions.</p>

                        <p>If we are familiar with Binary Space Partitioning or quad trees, this is exactly the same idea.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl19.png" alt="drl19" style="width:100%">
                            </div>
                        </div>

                        <p>Now we may be wondering this short of discretization makes sense in special domains like grid world.
                            <I>What about other state spaces? </I>
                        </p>

                        <p>Let's look at different domains. Most cars these days have automatic transmission, have we ever wondered how the car decides to pick what gear to switch to and when? Above is the simplified plot of how fuel consumption varies with
                            speed for different gears in a typical car. Let's assume our state only consisits of the vehicle's speed and which gear we are in and our reward is inversely proportional to fuel consumption. The actions available to the agent
                            are essentially switching up or down. Although speed a continuous value, it can be discretized into ranges such that a single gear is the most optimal in each range. Note that these ranges can be of different lengths, i.e.,
                            the discretization is non-uniform. If there we other dimensions to the state space such as throttle position, then they could be subdivided non-uniformly as well.</p>
                    </div>

                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Tile Coding</h2>
                    </div>
                    <p>If we have prior knowledge about the state space, we can manually design an appropriate discretization scheme. Like in our gear switching example, we knew the relationship between fuel consumption and speed. But, in order to function
                        in an arbitrary environments, we need a more generic method. One elegant approach for this is <strong>Tile Coding</strong>.</p>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl20.png" alt="drl20" style="width:100%">
                        </div>
                        <div class="column">
                            <img src="img/drl21.png" alt="drl21" style="width:100%">
                        </div>
                    </div>
                    <p>Here the underlying state space is continuous and 2-dimensional. We overlay multiple grids or tiling on top of the space, each slightly offset from each other. Any position S in the state space can be coercely identified by the tiles
                        it activates. If we assign a bit to each tile, then we can represent our new discretized state as a bit vector, with 1s for the tiles that get activated and 0s elsewhere. This by itself is a very efficient representation.</p>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl22.png" alt="drl22" style="width:100%">
                        </div>
                    </div>
                    <p>But the genius lies in how the state value function is computed using this scheme. Instead of storing separate value for each state V(s), it is defined in terms of the bit vector for that state and a weight for each tile. The tile-coding
                        algorithm in turn updates these weights iteratively. This ensures nearby locations that share tiles also share some component of state value, effectively smoothing the learned value function. </p>
                    <p>Tile Coding does have some drawbacks. Just like a simple grid-Based approach, we have to manually select the tile sizes, their offsets, number of tiling, etc. ahead of time.</p>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl23.png" alt="drl23" style="width:100%">
                        </div>
                    </div>
                    <p>A more flexible approach is <strong>adaptive tile coding</strong> which starts with fairly large tiles and divide each tile into whenever appropriate. <b><I>How do we know when to split?</I></b> We can use a heuristic for that. Basically
                        we want to split the state space when we realize that we are no longer learning much with the current representation, i.e, when our value function isn't changing. We can stop when we have reached some upper limit on number of splits
                        or some max iteration. In order to figure out which tile to split, we have to look at which one is likely to have the greatest affect on the value function. For this we need to keep track of sub-tiles and their projected weights.
                        Then we can pick the tile with the greatest difference between sub-tile weights. There are many other heuristics that we can use but the main advantage of adaptive tile coding is that it doesn't rely on a human to specify a discretization
                        ahead of time. The resulting state space is appropriately partitioned based on its complexity.</p>

                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Coarse Coding</h2>
                        <p>Coarse Coding is just like tile coding but
                            <U>uses a sparser set of feature to encode the state space</U>.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl24.png" alt="drl24" style="width:100%">
                            </div>
                        </div>
                        <p>Imagine dropping a bunch of circles on our 2D continuous state space. Take any state S which is a position in the space and marks all the circles it belongs to. Prepare a bit vector with the 1 for those circles and 0 for the rest.
                            And that's our sparse coding representation of our state. Looking at a 2D space helps us to visualize the basic idea but it also extends to the higher dimensions where circles become spheres and hyperspheres.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl25.png" alt="drl25" style="width:100%">
                            </div>
                        </div>
                        <p>There are some neat properties of coarse coding. Using smaller circles result in less generalization across the space. The learning algorithm has to work a bit longer but we have greater effective resolution. Larger circles lead
                            to more generalization and in general a smoother value function. We can use fewer large circles to cover the space thus reducing our representation but we would loose some resolution. It's not just the size of these circles
                            that we can vary, we can change them in other ways like making them taller or wider to get more resolution along one dimension versus the other.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl26.png" alt="drl26" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl27.png" alt="drl27" style="width:100%">
                            </div>
                        </div>
                        <p>In fact, this same technique generalizes to pretty much any shape. In coarse coding just like in tile coding, our resulting state representation is a binary vector. Think of each tile or cicle as a feature - 1 if it is active,
                            0 if it is not. A natural extension to this idea is to use the distance from the center of each circle as a measure of how active that feature is. This measure or response can be made to fall off smoothly using a gaussian or
                            bell-shaped curve centered on the circle which is known as a <strong>Radial Basis Functions</strong>. Of course, the resulting feature values will no longer be discrete, so we will end up with yet another continuous state vector
                            but what's interesting is that the number of features can be drastically reduced this way. </p>
                    </div>
                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Function Approximation</h2>
                        <ul>
                            <li>
                                <p>Given a problem domain with continuous states <span class="mathquill ud-math">s \in \mathcal{S} = {\mathbb{R}^{n}}</span>, we wish to find a way to represent the value function <span class="mathquill ud-math">v_{\pi}(s)</span>                                    (for prediction) or <span class="mathquill ud-math">q_{\pi}(s, a)</span> (for control).</p>
                                <p>We can do this by choosing a parameterized function that <em>approximates</em> the true value function:</p>
                                <p><span class="mathquill ud-math">\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)</span>
                                    <br />
                                    <br />
                                    <span class="mathquill ud-math">\hat{q}(s, a, \mathbf{w}) \approx q_{\pi}(s, a)</span></p>
                                <p>Our goal then reduces to finding a set of parameters <span class="mathquill ud-math">\mathbf{w}</span> that yield an optimal value function. We can use the general reinforcement learning framework, with a Monte-Carlo or
                                    Temporal-Difference approach, and modify the update mechanism according to the chosen function.</p>
                            </li>
                        </ul>
                        <p>So far we have looked at ways to discretize continuous state spaces. This enables us to use existing RL algorithms with little or no modifications.
                            <I>But there are some limitations!!</I>
                        </p>
                        <p>When the underlying space is complicated then number of discrete states needed can become very large. Thus we loose the advantage of discretization. Moreover, if we think about positions in the state space that are nearby we would
                            expect their values to be similar or smoothly changing. Discretization doesn't always exploit this characteristic, failing to generalize well across the space. </p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl281.png" alt="drl281" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl28.png" alt="drl28" style="width:100%">
                            </div>
                        </div>
                        <p>What we are after is the true state value function <span class="mathquill ud-math">v_{\pi}</span> or action value function <span class="mathquill ud-math">q_{\pi}</span> which is typically smooth and continuous over the entire
                            space. As we can imagine, capturing this completely is practically infeasible except for some very simple problems.</p>

                        <p>Our best hope is <strong>function approximation</strong>. It's still an approximation because we don't know what the true underlying function is. A general way to define such approximation is to introduce a parameter vector w that
                            shapes the function. Our task then reduces to tweaking this parameter vector till we find the desired approximation.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl29.png" alt="drl29" style="width:100%">
                            </div>
                        </div>
                        <p>Note that the approximating function can either map a state to its value or s (state, action) to the corresponding Q value. Another form is where we map from one state to a number of different Q values, one for each action all
                            at once. This is especially useful for Q-Learning as we see later.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl30.png" alt="drl30" style="width:100%">
                            </div>
                        </div>

                        <p>Let's focus on the first case. Approximating a state value function. We have a box in the middle which that is supposed to do some magic and convert the state s and parameter W into a scalar value! But how?</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl31.png" alt="drl31" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl32.png" alt="drl32" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl33.png" alt="drl33" style="width:100%">
                            </div>
                        </div>
                        <p>The first thing we need to do is to ensure we have a vector reprenting the state. Our state might already be a vector, in which case we don't need to do anything.
                            <br /> In general, we defines a transformation that converts any given state s into a feature vector <span class="mathquill ud-math">\mathbf{x}(s)</span>. This also gives us more flexibility since we don't have to operate on the
                            raw state values. We can use any computed or derived feature instead. So, we now have a feature vector <span class="mathquill ud-math">\mathbf{x}(s)</span> and a parameter vector <b>W</b> and we want a scalar value. What do
                            we do when we have two vectors and we want to produce a scalar? <strong>Dot Product!!</strong> This is the same as computing the linear combination of features. Multiply each feature with the corresponding weight and sum it
                            up. This is known as <strong>Linear Function Approximation</strong>, i.e. we are trying to approximate the underlying value function with the linear function.</p>
                    </div>
                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Linear Function Approximation</h2>
                        <p>How to estimate the parameter w?</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl34.png" alt="drl34" style="width:100%">
                            </div>
                        </div>
                        <p>As we have seen already, a linear function is a simple sum over all the features multiplied by their corresponding weights. Let's assume we have initialized these weights randomly and computed the value of a state <span class="mathquill ud-math">\hat{v}</span>.
                            How would we tweak w to bring the approximation closer and closer to the true function? Sounds like a numerical optimization problem! Let's use the gradient descent to find the optimal parameter vector. </p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl35.png" alt="drl35" style="width:100%">
                            </div>
                        </div>
                        <p>Firstly note that since <span class="mathquill ud-math">\hat{v}</span> is a linear function, its derivative with respect to w is simple the feature vector <span class="mathquill ud-math">\mathbf{x}(s)</span>. This is the nice thing
                            about the linear functions and why they are so popular. Now, let's think about what we are trying to optimize? We said we want to reduce or minimize the difference between the true value function <span class="mathquill ud-math">v_{\pi}</span>                            and the approximate value function <span class="mathquill ud-math">\hat{v}</span>. Let's write that down as a square difference since since we are not concerned with the sign of the error and we simply want to drive the difference
                            down towards 0. To be more accurate, since RL domains are typically stochastic, this is the expected squared error. So, we now have an objective function to minimize. To do that using the gradient descent, let's find the gradient
                            or derivative of the function with respect to w. Using the chain rule of differentiation, we get <strong>-2*(value difference)*(derivative of <span class="mathquill ud-math">\hat{v}</span>, which we earlier noted was simple the feature vector <span class="mathquill ud-math">\mathbf{x}(s)</span>)</strong>.
                            Note that we removed the expectation operator here to focus on the error gradient indicated by a single state s which we assumed has been chosen stochastically. If are able to sample enough states, we can come close to the
                            expected value. Let's plug this in to the general form of a gradient descent update rule with α(alpha) as the step size or learning rate parameter. Note that the -1/2 here is just to cancel out the -2 we got in the derivative.
                            This is the basic formulation we will use to iteratively reduce the error for each sample state till the approximate and true function are almost equal.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl36.png" alt="drl36" style="width:100%">
                            </div>
                        </div>
                        <p>Here is an intuitive explanation of how gradient descent optimizes the parameter vector. In each iteration, we change the weights a small step away from the error direction. So, the feature vector here points out what direction
                            is bad so we can move away from it.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl37.png" alt="drl37" style="width:100%">
                            </div>
                        </div>
                        <p>So far we have only been talking about approximating the state value function. In order to solve a model-free control problem, i.e., to take actions in an unknown environment, we need to approximate the action value function. We
                            can do this defining a feature transformation that utilizes both the state and action. Then we can use the same gradient descent method as we did for the state value function. </p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl38.png" alt="drl38" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl39.png" alt="drl39" style="width:100%">
                            </div>
                        </div>
                        <p>Finally let's look at the case where we reach the approximation function to compute all the action values at once. We can think of this as producing an action vector. For this purpose we can continue to use the same feature transformation
                            as before taking in both the state and action. But now how do we generate the different action values? One way of think about it is that we are trying to find m different action value functions, one for each action dimension.
                            But intuitively we know that these functions are related, so it makes sense to compute them together. We can do this by extending our weight vector and turning it into a matrix. Each column of the matrix emulates a separate
                            linear function. But the common features computed the state and action keep these functions tied to each other. If we have a problem domain with the continuous state space but a discrete action space which is very common, we
                            can easily select the action with the maximum value. Without this sort of parallel processing, we would have to pass each action one by one and then find their maximum. If our action space is also continuous, then this form
                            allows us to output more than a single value at once. For eg, If we were driving a carwould want to control both steering and throttle at the same time. </p>
                    </div>
                    <div class="row">
                        <div class="column">
                            <img src="img/drl40.png" alt="drl40" style="width:100%">
                        </div>
                    </div>
                    The primary limitation of linear function approximation is that we can only represent linear relationships between inputs and ouputs. With 1D input, this is basically a line, in 2D it becomes a plane and so on. What if our underlying value function has
                    a non-linear shape? A linear approximation may give a very bad result, that's when we need to start looking at non-linear functions.
                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Kernel Functions</h2>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl41.png" alt="drl41" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl42.png" alt="drl42" style="width:100%">
                            </div>
                        </div>
                        <p>A simple extension to linear function approximation can help us capture non-linear relationships. At the heart of this approach is the feature transformation. Remember how we defined it a generic sense, something that takes a state
                            or (state, action) pair and produces a feature vector. Each element of this vector can be produced by a separate function which can be non-linear. For eg., let's assume our state s is a single real number. Then we can define
                            say <span class="mathquill ud-math">x_{1}(s)</span> = s, <span class="mathquill ud-math">x_{2}(s)</span> = s², <span class="mathquill ud-math">x_{3}(s)</span> = s³ and so on. These are called kernel functions or basis functions.
                            They transform the input space into a different space. But note that since our value function is still defined as a linear combination of these feature, we can still use linear function approximation. What this allows the value
                            function to do is represent the non-linear relationships between the input states and output values.</p>

                        <div class="row">
                            <div class="column">
                                <img src="img/drl43.png" alt="drl43" style="width:100%">
                            </div>
                        </div>
                        <p>Radial basis functions are a very common form of kernels used for this purpose. Essentially think of the current state s as a location in the continuous state space, here depicted as a rectangular plane. Each basis function is
                            shown as a blob. The closer the state is to the center of the blob, the higher the response returned by the function and the further we go the response falls off gradually with the radius, hence the name radial basis function.
                            </p>
                        <p>Mathematically, this can be achieved by associating a gaussian kernel with each basis function, with its mean serving as a center of the blob and standard deviation determining how sharpy or smoothly the response falls off. So,
                            for any given state we can reduce the state representation into a vector of responses from these radial basis functions. </p>
                    </div>
                </div>

                <div class="divider"></div>
                <div class="ud-atom">
                    <h3></h3>
                    <div>
                        <h2 id="-td-prediction-action-values">Non-Linear Function Approximation</h2>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl44.png" alt="drl44" style="width:100%">
                            </div>
                        </div>
                        <p>Recall from the previous section how we can capture non-linear relationships between input state and output values using arbitrary kernels like radial basis functions as our feature transformation. In this model, our output value
                            is still linear with respect to the features. What if our underlying value function was truly non-linear with respect to a combination of these feature values.</p>
                        <div class="row">
                            <div class="column">
                                <img src="img/drl45.png" alt="drl45" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/drl46.png" alt="drl46" style="width:100%">
                            </div>
                        </div>
                        <p>To capture such complex relationships, let's pass our linear response obtained using the dot product through some non-linear function f. This is the basis of artificial neural networks. Such a non-linear function is generally called
                            an <strong>activation function</strong> and immensely increases the representational capacity of our approximator. We can iteratively update the parameters of any such function using gradient descent, learning rate α times
                            value difference times the derivative of the function with respect to the weights.</p>
                    </div>
                </div>
            </div>

            <footer class="footer">
                <div class="container">
                    <div class="row">
                        <div class="col-12">
                            <p class="text-center">
                                <a href="#" target="_blank">&copy 2024 <strong>Anubhav Singh</strong></a>
                            </p>
                        </div>
                    </div>
                </div>
            </footer>

        </div>
    </div>



    <script src="../../assets/js/jquery-3.3.1.min.js"></script>
    <script src="../../assets/js/plyr.polyfilled.min.js"></script>
    <script src="../../assets/js/bootstrap.min.js"></script>
    <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
    <script src="../../assets/js/katex.min.js"></script>
    <script>
        // Initialize Plyr video players
        const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));
    
        // render math equations
        let elMath = document.getElementsByClassName('mathquill');
        for (let i = 0, len = elMath.length; i < len; i += 1) {
          const el = elMath[i];
    
          katex.render(el.textContent, el, {
            throwOnError: false
          });
        }
    
        // this hack will make sure Bootstrap tabs work when using Handlebars
        if ($('#question-tabs').length && $('#user-answer-tabs').length) {
          $("#question-tabs a.nav-link").on('click', function () {
            $("#question-tab-contents .tab-pane").hide();
            $($(this).attr("href")).show();
          });
          $("#user-answer-tabs a.nav-link").on('click', function () {
            $("#user-answer-tab-contents .tab-pane").hide();
            $($(this).attr("href")).show();
          });
        } else {
          $("a.nav-link").on('click', function () {
            $(".tab-pane").hide();
            $($(this).attr("href")).show();
          });
        }
    
        // side bar events
        $(document).ready(function () {
          $("#sidebar").mCustomScrollbar({
            theme: "minimal"
          });
    
          $('#sidebarCollapse').on('click', function () {
            $('#sidebar, #content').toggleClass('active');
            $('.collapse.in').toggleClass('in');
            $('a[aria-expanded=true]').attr('aria-expanded', 'false');
          });
        });
    </script>
</body>

</html>