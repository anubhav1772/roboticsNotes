<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Summary</title>
  <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
  <link rel="stylesheet" href="../../assets/css/plyr.css">
  <link rel="stylesheet" href="../../assets/css/katex.min.css">
  <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
  <link rel="stylesheet" href="../../assets/css/styles.css">
  <style type="text/css">
    /* Three image containers (use 25% for four, and 50% for two, etc) */
.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

  </style>
</head>

<body>
  <div class="wrapper">
    <nav id="sidebar">
  <div class="sidebar-header">
    <h3>Temporal-Difference Methods</h3>
  </div>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../../index.html" class="article">Back to Home</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled components">
    <li class="">
      <a href="#">01. Introduction</a>
    </li>
    <li class="">
      <a href="#">02. Monte-Carlo Review</a>
    </li>
    <li class="">
      <a href="#">03. TD Prediction: TD(0)</a>
    </li>
    <li class="">
      <a href="#">04. TD Prediction: Action Values</a>
    </li>
    <li class="">
      <a href="#">05. TD Control: Sarsa(0)</a>
    </li>
    <li class="">
      <a href="#">06. TD Control: Sarsamax</a>
    </li>
    <li class="">
      <a href="#">07. TD Control: Expected Sarsa</a>
    </li>
    <li class="">
      <a href="#">08. Analyzing Performance</a>
    </li>
  </ul>

  <ul class="sidebar-list list-unstyled CTAs">
    <li>
      <a href="../../index.html" class="article">Back to Home</a>
    </li>
  </ul>
</nav>

    <div id="content">
      <header class="container-fluild header">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <div class="align-items-middle">
                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                  <div></div>
                  <div></div>
                  <div></div>
                </button>

                <h1 style="display: inline-block">Temporal-Difference Methods</h1>
              </div>
            </div>
          </div>
        </div>
      </header>

      <main class="container">
        <div class="row">
          <div class="col-12">
            <div class="ud-atom">
  <h3></h3>
  <!-- <div>
  <h1 id="summary">Summary</h1>
</div> -->

</div>
<!-- <div class="divider"></div><div class="ud-atom">
  <h3></h3> -->
  <div>
  <figure class="figure">
    <img src="img/screen-shot-2017-10-17-at-11.02.44-am.png" alt="The cliff-walking task (Sutton and Barto, 2017)" style="width:60%" class="img img-fluid">
    <figcaption class="figure-caption">
      <p>The cliff-walking task (Sutton and Barto, 2017)</p>
    </figcaption>
  </figure>
</div>


</div>
</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="review">Monte-Carlo Review</h2>
  <br />
  <p>Monte-Carlo methods needed the episode to end so that the return could be calculated and then used as an estimate for the action values.</p>

  <p>For instance, in the self-driving car (estimating if the car is likely to crash) - the Monte-Carlo approach would have the car crashed everytime it wants to learn anything. This is too expensive and quite dangerous.</p>

  <p>TD learning will solve this problem. Instead of waiting to update the value when the interaction ends, it will amend its prediction at every step. It can be used to solve both continuous and episodic tasks.</p>
  <br />
<div>
  <figure class="figure">
    <img src="img/mc_review.png" alt="" style="width:60%" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>
</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-td-prediction-td0">TD Prediction: TD(0)</h2>
<ul>
<li>Whereas Monte Carlo (MC) prediction methods must wait until the end of an episode to update the value function estimate, temporal-difference (TD) methods update the value function after every time step.</li>
<li>For any fixed policy, <strong>one-step TD</strong> (or <strong>TD(0)</strong>) is guaranteed to converge to the true state-value function, as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small.</li>
<li>In practice, TD prediction converges faster than MC prediction.</li>
</ul>
</div>
<div class="row">
  <div class="column">
    <img src="img/td1.png" alt="td1" style="width:100%">
  </div>
  <div class="column">
  <p>Here, we now have an update step that understands the value of a state in terms of the values of its successive states.</p>
  <p>Why do we want to do that anyway?</p>
  </div>
  <div class="column">
    <p>The first thing to notice is that we removed any mention of the return that comes at the end of the episode. And, in fact this new update step gives us a way to update the state value after every time step.</p>
  </div>
</div>
<div class="row">
  <div class="column">
    <img src="img/td2.png" alt="td2" style="width:100%">
  </div>
  <div class="column">
    <img src="img/td3.png" alt="td3" style="width:100%">
  </div>
  <div class="column">
    <img src="img/td4.png" alt="td4" style="width:100%">
  </div>
  <div class="column">
    <img src="img/td5.png" alt="td5" style="width:100%">
  </div>
  <div class="column">
    <img src="img/td6.png" alt="td6" style="width:100%">
  </div>
</div>
<div class="row">
  <div class="column">
    <img src="img/td7.png" alt="td7" style="width:100%">
  </div>
  <div class="column">
    <img src="img/td8.png" alt="td8" style="width:100%">
  </div>
</div>
</div>
<!-- <div class="divider"></div> --><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/td-prediction.png" alt="" style="width:60%" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>
<div class="ud-atom">
  <h3></h3>
  <div>
  <p>TD(0) is <strong>guaranteed to converge</strong> to the true state-value function, as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small.  If you recall, this was also the case for constant-<span class="mathquill ud-math">\alpha</span> MC prediction.  However, TD(0) has some nice advantages:</p>
<ul>
<li>Whereas MC prediction  must wait until the end of an episode to update the value function estimate, TD prediction methods update the value function after every time step.  Similarly, TD prediction methods work for continuous and episodic tasks, while MC prediction can only be applied to episodic tasks.</li>
<li>In practice, TD prediction converges faster than MC prediction.  (<em>That said, no one has yet been able to prove this, and it remains an open problem.</em>) For an example of how to run this kind of analysis, check out Example 6.2 in the <a href="http://go.udacity.com/rl-textbook" target="_blank">textbook</a>.</li>
</ul>
</div>

</div>
</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-td-prediction-action-values">TD Prediction: Action Values</h2>
<ul>
<li>In this concept, we discussed a TD prediction algorithm for estimating action values.  </li>
<li>Similar to TD(0), this algorithm is guaranteed to converge to the true action-value function, as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small.</li>
</ul>
</div>
<div class="row">
  <div class="column">
    <img src="img/td_pred.png" alt="td_pred" style="width:100%">
  </div>
  <div class="column">
    <img src="img/tdav1.png" alt="tdav1" style="width:100%">
  </div>
</div>
<p>The agent interacts with the environment. At time t=0, it receives some state S0, then it uses the policy to pick an action A0, immediately afterward the agent receives a reward R1 and next state S1. At this point the agent uses its experience to update its estimate for the value of the state from time t=0. At the next point in time (t=1), the agent chooses an action again by consulting the policy, then it receives a reward and next state. Then it uses that information to update the value of state from time t=1. Then the process continues with the agent always consults the same policy to pick an action, recieves a reward and next state and then update the value function.  </p>
<p>The question is <I>how might we adapt this process to instead return an estimate of the action values?</I></p>
<div class="row">
  <div class="column">
    <img src="img/tdav2.png" alt="tdav2" style="width:100%">
  </div>
  <div class="column">
    <img src="img/tdav3.png" alt="tdav3" style="width:100%">
  </div>
</div>
<p>Well, intstead of having an update equation that relates the values of successive states, what we instead need to do is having an update equation that relates the values of successive (state, action) pairs. Then instead of updating the values after each state is received, the agent instead update the values after each action is chosen. If the agent interacts with the environment long enough, it will have a pretty much estimate of the action value function.  </p>
</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-td-control-sarsa0">TD Control: Sarsa(0)</h2>

<p>How might an agent determine an optimal policy?</p>
</div>
<div class="row">
  <div class="column">
    <img src="img/sarsa1.png" alt="sarsa1" style="width:100%">
  </div>
</div>
<div class="row">
  <div class="column">
    <img src="img/sarsa2.png" alt="sarsa2" style="width:100%">
  </div>
  <div class="column">
    <img src="img/sarsa3.png" alt="sarsa3" style="width:100%">
  </div>
</div>
<p>We build off the algorithm that we used to estimate the action value function. In that case, after each action is selected, the agent update its estimate and it's important to note that the agent uses the same policy at every time step to select the actions. But, now to adapt this to produce a control algorithm we will gradually change the policy so let it become more optimal at every time step. One of the method we use for this pretty much similar to what we used in the Monte-Carlo case, where we select the action at every time step by using a policy, that's Ɛ-greedy with respect to the current estimate of the action values.</p>

<p>At the initial time step we begin by setting Ɛ to 1. Then A0 and A1 are chosen according to the equal probable random policy. Then at all future time steps after an action is chosen, we update the action value function and construct the corresponding Ɛ-greedy policy. As long as we specify an appropriate values for Ɛ, the algorithm is guaranteed to converge to the optimal policy. The name of the algorithm is <strong>Sarsa(0)</strong>, also known as <strong>Sarsa</strong> for short. The name comes from the fact that each action value update uses a (state, action, reward, next state, next action) tuple of interaction.</p>

<p><strong>Sarsa(0)</strong> (or <strong>Sarsa</strong>) is an on-policy TD control method.  It is guaranteed to converge to the optimal action-value function <span class="mathquill ud-math">q_*</span>, as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small and <span class="mathquill ud-math">\epsilon</span> is chosen to satisfy the <strong>Greedy in the Limit with Infinite Exploration (GLIE)</strong> conditions.</p>
</div>
<!-- <div class="divider"></div> --><div class="ud-atom">
  <h3></h3>
  <div>
  <figure class="figure">
    <img src="img/sarsa.png" alt="" style="width:60%" class="img img-fluid">
    <figcaption class="figure-caption">
      
    </figcaption>
  </figure>
</div>
<div>
  <p>Sarsa(0) is <strong>guaranteed to converge</strong> to the optimal action-value function, as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small, and the <strong>Greedy in the Limit with Infinite Exploration (GLIE)</strong> conditions are met.  The GLIE conditions were introduced in the previous section, when we learned about MC control.  Although there are many ways to satisfy the GLIE conditions, one method involves <strong>gradually decaying the value of <span class="mathquill ud-math">\epsilon</span> when constructing <span class="mathquill ud-math">\epsilon</span>-greedy policies.</strong></p>
<p>In particular, let <span class="mathquill ud-math">\epsilon_i</span> correspond to the <span class="mathquill ud-math">i</span>-th time step.  Then, if we set <span class="mathquill ud-math">\epsilon_i</span> such that:</p>
<ul>
<li><span class="mathquill ud-math">\epsilon_i > 0</span> for all time steps <span class="mathquill ud-math">i</span>, and </li>
<li><span class="mathquill ud-math">\epsilon_i</span> decays to zero in the limit as the time step <span class="mathquill ud-math">i</span> approaches infinity (that is, <span class="mathquill ud-math">\lim_{i\to\infty} \epsilon_i = 0</span>),</li>
</ul>
<p>then the algorithm is guaranteed to yield a good estimate for <span class="mathquill ud-math">q_(s, a)</span>, as long as we run the algorithm for long enough. </p> 

<p>A corresponding optimal policy <span class="mathquill ud-math">\pi_*</span> can then be quickly obtained by setting <span class="mathquill ud-math">\pi_*<em>(s) = \arg\max_{a\in\mathcal{A}(s)} q_</em>(s, a)</span> for all <span class="mathquill ud-math">s\in\mathcal{S}</span>.</p>
</div>
</div>
<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-td-control-sarsamax">TD Control: Sarsamax</h2>
<ul>
<li><strong>Sarsamax</strong> (or <strong>Q-Learning</strong>) is an off-policy TD control method</li>
</ul>
</div>
<div class="row">
  <div class="column">
    <img src="img/sarsa_review.png" alt="sarsa_review" style="width:100%">
  </div>
</div>
<p>Remember in the Sarsa algorithm we begin by initializing all action value to 0 and construction the corresponding Ɛ-greedy policy. Then the agent begins interacting with the environment and receives the first state (S0). Next it used the policy to choose its action (A0), immediately afterward it receives a reward (R0) and next state (S1). Then the agent uses the same policy to pick next action (A1). After choosing that action, it updates the action value corresponding to the previous (state, action) pair and improves the polciy to be Ɛ-greedy with respect to the most recent estimate of the action values.

We build off of this algorithm to design an another control algorithm that works slight differently. This algorithm is called <strong>Sarsamax</strong> but it's also known as <strong>Q-Learning</strong>.</p>
<div class="row">
  <div class="column">
    <img src="img/sarsamax1.png" alt="sarsamax1" style="width:100%">
  </div>
  <div class="column">
    <img src="img/sarsamax2.png" alt="sarsamax2" style="width:100%">
  </div>
  <div class="column">
    <img src="img/sarsamax3.png" alt="sarsamax3" style="width:100%">
  </div>
</div>
<p>We still begin with the same initial values for the action value and the policy. The agent receives the initial state (S0), the first action (A0) is still chosen from the initial policy but then after receiving the reward (R1) and next state (S1), we are going to something else, namely we will update the policy before choosing the next action.<br /><br />

In the Sarsa case, our update step was one step later and plugged in the action that was selected using the Ɛ-greedy policy and for every step of the algorithm it was the case that all of the actions we use for updating the action values exactly coincide with those we were experienced by the agent. But, in general this doesn't have to be the case. In particluar, consider using the action from the greedy policy instead of the Ɛ-greedy policy. This is what Sarsamax or Q-Learning does, in which we rely on the fact that the greedy action corresponding to a state is just the one that maximizes the action values for that state.<br /><br />

So what happens is after we update the action value for the time step 0 using the greedy action, we then select A1 using the Ɛ-greedy policy corresponding to the action values we just updated. This continues where we receive a reward and next state. Then we do the same thing we did before where we update the value corresponding to S1 and A1 using the greedy action. Then we select A2 using the corresponding Ɛ-greedy policy.</p>
<div class="row">
  <div class="column">
    <img src="img/sarsamax4.png" alt="sarsamax4" style="width:100%">
  </div>
</div>
<p>To understand precisely what this update step is doing we compare it to the correspongding step in the Sarsa algorithm. In Sarsa, the update step pushes the action values closer to evaluating whatever Ɛ-greedy policy is currently being followed by the agent. And it's possible to show that Sarsamax instead directly attempt to approximate the optimal value function at every time step.</p>

<img src="img/sarsamax.png" alt="" style="width:60%" class="img img-fluid"/>
<p>Sarsamax (or Q-Learning) is <strong>guaranteed to converge</strong> to the optimal action value function <span class="mathquill ud-math">q_*</span>, under the same conditions that guarantee convergence of the Sarsa control algorithm.</p>
</div>
</div>


<div class="divider"></div><div class="ud-atom">
  <h3></h3>
  <div>
  <h2 id="-td-control-expected-sarsa">TD Control: Expected Sarsa</h2>
<ul>
<li><strong>Expected Sarsa</strong> is an on-policy TD control method.  </li>
<li>It is guaranteed to converge to the optimal action value function <span class="mathquill ud-math">q_*</span>, under the same conditions that guarantee convergence of Sarsa and Sarsamax.</li>
</ul>
<p>Closely resembles the Sarsamax where the only difference is in the update step for the action values.</p> 
<p>Remember that the Sarsamax or Q-Learning took the maximum over all actions of all possible next (state, action) pairs. In other words, it chooses what values to place here by pluggin in the one action that maximizes the action value estimate corresponding to the next state.</p>
<p>Expected Sarsa, on the other hand, does a bit different. It uses the expected value of the next (state, action) pair where the expectation takes into account the probability that agent selects each possible action from the next state.</p>
</div>
<img src="img/expected_sarsa.png" alt="" style="width:60%" class="img img-fluid"/>
<img src="img/expected-sarsa.png" alt="" style="width:60%" class="img img-fluid"/>

<p>Expected Sarsa is <strong>guaranteed to converge</strong> under the same conditions that guarantee convergence of Sarsa and Sarsamax.  </p>
<p>Remember that <strong><em>theoretically</em></strong>, the as long as the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small, and the <strong>Greedy in the Limit with Infinite Exploration (GLIE)</strong> conditions are met, the agent is guaranteed to eventually discover the optimal action-value function (and an associated optimal policy).</p>  <p>However, <strong><em>in practice</em></strong>, for all of the algorithms we have discussed, it is common to completely ignore these conditions and still discover an optimal policy.</p>
</div>
<div class="divider"></div>

<div class="ud-atom">
  <div>
  <h2 id="-analyzing-performance">Analyzing Performance</h2>
<ul>
<li>On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Q-learning). </li>
<li>Expected Sarsa generally achieves better performance than Sarsa.</li>
</ul>
</div>

<p>All of the TD control algorithms we have examined (Sarsa, Sarsamax, Expected Sarsa) converge to the optimal action-value function <span class="mathquill ud-math">q_*</span> (and so yield the optimal policy <span class="mathquill ud-math">\pi_*</span>) if 
    <ol>
      <li>the value of <span class="mathquill ud-math">\epsilon</span> decays in accordance with the GLIE conditions, and</li>
      <li>the step-size parameter <span class="mathquill ud-math">\alpha</span> is sufficiently small.</li>
    </ol></p>
<p>The differences between these algorithms are summarized below:</p>
<ul>
<li>Sarsa and Expected Sarsa are both <strong>on-policy</strong> TD control algorithms.  In this case, the same (<span class="mathquill ud-math">\epsilon</span>-greedy) policy that is evaluated and improved is also used to select actions.</li>
<li>Sarsamax is an <strong>off-policy</strong> method, where the (greedy) policy that is evaluated and improved is different from the (<span class="mathquill ud-math">\epsilon</span>-greedy) policy that is used to select actions.</li>
<li>On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Sarsamax). </li>
<li>Expected Sarsa generally achieves better performance than Sarsa.</li>
</ul>
<p>If you would like to learn more, you are encouraged to read Chapter 6 of the <a href="http://go.udacity.com/rl-textbook" target="_blank">textbook</a> (especially sections 6.4-6.6).</p>


<img src="img/screen-shot-2017-12-17-at-12.49.34-pm.png" alt="" class="img img-fluid"/>
<p>The figure shows the performance of Sarsa and Q-learning on the cliff walking environment for constant <span class="mathquill ud-math">\epsilon = 0.1</span>.  As described in the textbook, in this case,</p>
<ul>
<li>Q-learning achieves worse online performance (where the agent collects less reward on average in each episode), but learns the optimal policy, and</li>
<li>Sarsa achieves better online performance, but learns a sub-optimal "safe" policy.</li>
</ul>

</div>

<div class="divider"></div>
<!-- </div> -->

</main>
<footer class="footer">
  <div class="container">
    <div class="row">
      <div class="col-12">
        <p class="text-center">
          <a href="#" target="_blank">&copy 2024 <strong>Anubhav Singh</strong></a>
        </p>
      </div>
    </div>
  </div>
</footer>
</div>

</div>

  <script src="../../assets/js/jquery-3.3.1.min.js"></script>
  <script src="../../assets/js/plyr.polyfilled.min.js"></script>
  <script src="../../assets/js/bootstrap.min.js"></script>
  <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
  <script src="../../assets/js/katex.min.js"></script>
  <script>
    // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });
    });
  </script>
</body>

</html>
