<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Localization</title>
    <link rel="stylesheet" href="../../assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="../../assets/css/plyr.css">
    <link rel="stylesheet" href="../../assets/css/katex.min.css">
    <link rel="stylesheet" href="../../assets/css/jquery.mCustomScrollbar.min.css">
    <link rel="stylesheet" href="../../assets/css/styles.css">
    <link rel="stylesheet" href="../../assets/css/cc-icons.min.css">    <!-- Creative Commons Icons -->
    <link rel="shortcut icon" type="image/png" href="../../assets/img/robo-icon.png" />
    <style type="text/css">
        /* Three image containers (use 25% for four, and 50% for two, etc) */
      .column {
        float: left;
        width: 33.33%;
        padding: 5px;
      }
    
      /* Clear floats after image containers */
      .row::after {
        content: "";
        clear: both;
        display: table;
      }
    </style>
</head>

<body>
    <div class="wrapper">
        <nav id="sidebar">
            <div class="sidebar-header">
                <!-- <h3>Localization</h3> -->
                </br></br></br>
            </div>
            
            <ul class="sidebar-list list-unstyled CTAs">
                <li>
                    <a href="../../index.html" class="article">Back to Home</a>
                </li>
            </ul>

            <ul class="sidebar-list list-unstyled components">
                <li class="">
                    <a href="">01. Introduction to Localization</a>
                </li>
                <li class="">
                    <a href="">02. Kalman Filters</a>
                </li>
                <li class="">
                    <a href="">03. Monte Carlo Localization</a>
                </li>
            </ul>
            </ul>

            <ul class="sidebar-list list-unstyled CTAs">
                <li>
                   <a href="../../index.html" class="article">Back to Home</a>
                </li>
                <li>
                   <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" class="article"><i class="cc cc-SIX cc-4"></i></a>
                </li>
            </ul>
        </nav>

        <div id="content">
            <header class="container-fluild header">
                <div class="container">
                    <div class="row">
                        <div class="col-12">
                            <div class="align-items-middle">
                                <button type="button" id="sidebarCollapse" class="btn btn-toggle-sidebar">
                                    <div></div>
                                    <div></div>
                                    <div></div>
                                </button>

                                <h1 style="display: inline-block">Localization</h1>
                            </div>
                        </div>
                    </div>
                </div>
            </header>

            <main class="container">
                <div class="ud-atom">
                    <div>
                        <h2 id="review">Introduction to Localization</h2> Localization is the challenge of determining robot's pose in a mapped environment. We do this by implementing a probabilistic algorithm to filter noisy sensor measurement and track robot's position and orientation.

                        <div class="row">
                            <div class="column">
                                <img src="img/loc0.png" alt="loc0" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/loc1.png" alt="loc1" style="width:100%">
                            </div>
                            <div class="column">
                                <img src="img/loc2.png" alt="loc2" style="width:100%">
                            </div>
                        </div>
                        <br /> Here is an example of a robot inside of a mapped environment. This robot's job is keep house's floor clean. However, insead of starting at its charging dock, it's starting position is elsewhere in the room. The robot is moving
                        around, taking measurements trying to figure out where it could be positioned in the room. Since this is a probabilistic model, the robot might have a few guesses as to where it is located in the room. However, over time, hopefully,
                        it should narrowed down on a precise location. Robot's pose is X and Y positioned coordinates within the room and θ, its orientation.
                        <br />
                        <br />

                        <div class="row">
                            <div class="column">
                                <img src="img/loc3.png" alt="loc3" style="width:100%">
                            </div>
                            <div class="column">
                                <ul>
                                    <li>
                                        <strong>Extended Kalman Filter</strong> is the most common Gaussian Filter that helps in estimating the state of non-linear models.
                                    </li>
                                    <li>
                                        <strong>Markov Localization</strong> is a Bayes Filter localization algorithm which maintains the probability distribution over the set of all possible positions and orientations the robot might be located at.
                                    </li>
                                    <li>
                                        <strong>Grid Localization</strong> is referred to as Histogram Filter since it is capable of estimating the robot's pose using grids.
                                    </li>
                                    <li>
                                        <strong>Monte Carlo Localization</strong> is also known as Particle Filter because it estimates the robot's pose using particles.
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <br /> The amount of information present and the nature of the environement that a robot is operating in determines the difficulty of the localization tasks.
                        <ul>
                            <li><strong>Local Localization Problem</strong></li>
                            <ul>
                                <li>The easiest localization problem is called <strong>position tracking</strong>, also known as local localization.</li>
                                <li>In this, the robot knows its initial pose. Localization challenge entails estimating robot's pose as it moves around the environment.</li>
                                <li>Uncertainty is limited to the region surrounding the robot.</li>
                            </ul>
                            <li><strong>Global Localization Problem</strong></li>
                            <ul>
                                <li>In this case, the robot's initial pose is unknown and the robot must determine its pose relative to the ground truth map.</li>
                                <li>Uncertainty is much more compared to position tracking, making it a harder problem.</li>
                            </ul>
                            <li><strong>Kidnapped Robot Problem</strong></li>
                            <ul>
                                <li>The most challenging localization problem.</li>
                                <li>This problem is just like global localization, except that the robot may be kidnapped at any time and moved to a new location on the map.</li>
                                <li>Localization algorithms are not free from errors. There would be instances of robot miscalculating where it is. The kidnapped robot problem teaches the robot to recover from such instances and once again correctly locate
                                    it on the map. Doing so is critical to building robot's localization algorithms.</li>
                            </ul>
                        </ul>
                        In the real world, we have to consider where an environment is static or dynamic.
                        <div class="row">
                            <div class="column">
                                <img src="img/loc4.png" alt="loc4" style="width:100%">
                            </div>
                        </div>
                        <div>
                            <p><strong>Resources for Additional Localization Knowledge:</strong></p>
                            <ul>
                                <li>Textbook: <a href="http://www.probabilistic-robotics.org/" target="_blank">Probabilistic Robotics</a> by Sebastian Thrun,‎ Wolfram Burgard,‎ and Dieter Fox.</li>
                            </ul>
                        </div>
                    </div>
                    <div class="divider"></div>
                    <div class="ud-atom">
                        <h3></h3>
                        <div>
                            <h2 id="-td-prediction-td0">Kalman Filters</h2> Kalman Filter is an estimation algorithm, used to estimate the value of variable in real time as the data is being collected.
                            <p>The reason why KF is noteworthy is that it can take data with a lots of uncertainties or noise in the measurement and provides very accurate estimate of the real values.</p>
                            <img src="img/loc5.png" alt="loc5" style="width:60%" />
                            <br />
                            <p>KF is a continuous iteration of following two steps:</p>
                            <ol>
                                <li><strong>Measurement Update:</strong> The first step is the measurement update. We use the recorded measurement to update our state.</li>
                                <li><strong>State Prediction:</strong> The second step is the state prediction. We use the information we have about our current state to predict what the future state would be. At the start we use the
                                    <I>initial guess</I>.</li>
                            </ol>
                            <p>We continue to iterate through above two steps and it doesn't take many iterations for an estimate to converge on the real value. </p>
                            <p>Another way of looking at a Kalman Filter is just like we’d look at any other filter. What does it take as an input, what does it filter out, and what important substance does it let through? The graphic below compares a household
                                coffee filter, an engineering low-pass filter, and a Kalman filter.</p>
                            <img src="img/loc8.png" alt="loc8" style="width:60%" />
                            <br />
                            <p>KF is used to estimate the state of a system when the measurements are noisy.</p>
                            <p>There are three variation of KF:</p>
                            <ul>
                                <li><strong>Standard Kalman Filter:</strong> Applied to the linear systems where the output is proportional to the input. </li>
                                <li><strong>Extended Kalman Filter (EKF):</strong> Applied to non-linear systems. </li>
                                <ul>
                                    <li>More applicable in robotics where as the real world systems are more often non-linear than linear.</li>
                                </ul>
                                <li><strong>Unscented Kalman Filter (UKF):</strong> Appropriate for highly non-linear systems, where EKF may fail to converge.</li>
                            </ul>
                            <div class="row">
                                <div class="column">
                                    <img src="img/rob1.png" alt="rob1" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob2.png" alt="rob2" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob3.png" alt="rob3" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob4.png" alt="rob4" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob5.png" alt="rob5" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob6.png" alt="rob6" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob7.png" alt="rob7" style="width:100%">
                                </div>
                                <div class="column">
                                    <img src="img/rob8.png" alt="rob8" style="width:100%">
                                </div>
                            </div>
                            <br />
                            <p>Before we dive into the details of the Kalman Filter, it's important to understand few intricacies of robot operation that will get context that why a KF works the way it does. Let's explore two different robot worlds. The
                                ideal world and the real world. In both the worlds, the robots know their starting positions. In the ideal world, the robot is instructed to move 10m forward. The robot proceeds to do so and stops precisely 10m from its
                                starting position. The movement was error-free. The robot in the real world is also asked to move forward 10m. In the real world, however, there are few complexities that result in the robot movement being imprecise. The
                                robot may incounter imperfections in the terrains, experience wheel slip or be adversely affected by other factors in its environment. Upon completion of its movement, the robot may not be at the 10m mark precisely, but
                                some distance ahead of or behind its desired goal. This error would be different with every movement performed due to the randomness encountered in the environment.
                                <p>

                                    <p>Let's look at this movement on a graph. If we were to record a real world robot moving a 10m forward a total of 100 times, the result would look like data shown above. The data resembles a Bell curve, also known as
                                        a Gaussian. This graph displays a probability distribution of the robot's final position after multiple iterations of the movement. The X-axis represents the distance travelled by the robot and the Y-axis represents
                                        how often the robot stopped at that distance. This curve shows if we ask a robot to move, it is most likely to stop at the 10m mark, but at times it can find itself as far off as 12m mark.</p>

                                    <p>The shape of the Gaussian is specific to the robot in the environment it is operating in. If the robot is driving around a factory floor, there would be a fewer environment factors to affect it, so it's movement would
                                        be more precise and its distribution would be more narrow. On the other hand, if the robot is performing a rescue mission, it may have any number of environemental factors affect it, such as adverse weather or unexpected
                                        movement due to unstable terrain. The result is a much wider Gaussian distribution. But the distribution of the uncertainty is not the only problem here. If a robot were to continue to take blind movements one after
                                        the other, then it's location would be less and less certain with every movement. We can see this displayed on the graph. Every movement is uncerain and these uncertainties stacked up over time. With all these uncertainties
                                        in motion, our robot needs a way to sense its own action but unfortunately, sensory data is often uncertain too.</p>

                                    <p>Let's go back to our two robots world to explore this. Let's say we are interested in knowing the speed a robot and now we have sensor onboard for the robot to measure this. In an ideal world, the sensor would measure
                                        the speed of the robot accurately and response to change in the speed instantaneously. However, in the real world, the sensor measurement would contain some amount of noise. An expensive encoder may come close to
                                        what we would expect in an ideal world, but if we are using hobbyist grade robotic sensors, they will exhibit imperfect measurement.</p>
                                    <p>Advantages of Kalman Filters:</p>
                                    <ul>
                                        <li>KF can very quickly estimate the true value of variable being measured. Unlike other algorithms that require a lots of data to measure the estimate, the KF is able to do so after just few sensor measurement. It
                                            does so by using initial guess and by taking into account the expected uncertainty of a sensor or movement. (Both movement and sensory measuements are noisy in the real world!)</li>
                                        <li>
                                            <p>GPS is only accurate for few meters.
                                                <br /> Sensor Fusion -> uses KF (uses data from multiple sensors)</p>
                                        </li>
                                    </ul>
                        </div>
                        <h4 id="-1d-gaussian">1D Gaussian</h4> At the basis of the Kalman Filter is the Gaussian distribution, sometimes referred to as a bell curve or normal distribution. (Rover example) - after executing one motion, the rover’s location was represented by a Gaussian. It’s
                        exact location was not certain, but the level of uncertainty was bounded. It was unlikely that the rover would be more than a few meters away from its target location, and it would be nearly impossible for it to show up at the
                        50 meter mark.
                        <br />
                        <br />

                        <img src="img/loc6.png" alt="" style="width:60%" class="img img-fluid" />
                        <br />
                        <p>This is the role of a Kalman Filter - after a movement or a measurement update, it outputs a unimodal Gaussian distribution. This is its best guess at the true value of a parameter.</p>
                        <h4 id="-gaussian-distribution">Gaussian Distribution</h4>
                        <p>A Gaussian distribution is a probability distribution, which is a continuous function. The probability that a random variable, x, will take a value between <span class="mathquill ud-math">x_1</span> and <span class="mathquill ud-math">x_2</span>                            is given by the integral of the function from <span class="mathquill ud-math">x_1</span> to <span class="mathquill ud-math">x_2</span>. </p>
                        <div class="mathquill">p(x_1
                            < x < x_2)=\ int_{x_1}^{x_2}f_x(x)dx</div>
                                <p>In the image below, the probability of the rover being located between 8.7m and 9m is 7%.</p>
                                <img src="img/loc7.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />

                                <!-- <h4 id="mean-and-variance">Mean and Variance</h4> -->
                                <p>A Gaussian is characterized by two parameters - its mean (μ) and its variance (σ²). The mean is the most probable occurrence and lies at the centre of the function, and the variance relates to the width of the curve. The
                                    term unimodal implies a single peak present in the distribution.</p>
                                <p>Gaussian distributions are frequently abbreviated as N(x: μ, σ²)</p>


                                <p>The formula for the Gaussian distribution is:</p>
                                <div class="mathquill">p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</div>
                                <br />
                                <p> Notice that the formula contains an exponential of a quadratic function. The quadratic compares the value of x to μ, and in the case that x=μ, the exponential is equal to 1 (<span class="mathquill ud-math">e^0 = 1</span>).
                                    You’ll note here, that the constant in front of the exponential is a necessary normalizing factor.</p>

                                <p>Just like with discrete probability, like a coin toss, the probabilities of all the options must sum to one. Therefore, the area underneath the function always sums to one. </p>
                                <div class="mathquill">\int p(x)dx = 1</div>
                                <br />

                                <p><b>What is represented by a Gaussian distribution?</b></p>
                                <ul>
                                    <li>Predicted Motion</li>
                                    <li>Sensor Measurement</li>
                                    <li>Estimated State of Robot</li>
                                </ul>
                                <p>The Kalman Filter treats all noise as unimodal Gaussian. In reality, that’s not the case. However, the algorithm is optimal if the noise is Gaussian. The term optimal expresses that the algorithm minimizes the mean square
                                    error of the estimated parameters.</p>

                                <h4>Designing 1D Kalman FIlter</h4>
                                <br />
                                <p>Since the robot is unable to sense the world around it with complete certainty, it holds an internal belief which is its best guess at the state of the environment including itself.</p>

                                <p>The robot constrained to a plane can be described with three state variables, two coordinates X and Y to identify its positions and one angle yaw to identify its orientation. These are state variables, denoted by
                                    <span
                                    class="mathquill">x</span>. A robot can perceive its environment using sensors which produces a mesurement, denoted by <span class="mathquill">z</span>. Next are control actions such as movement can change the state of our environement,
                                        denoted by <span class="mathquill">u</span>.</p>

                                <h6 id="variable-naming-conventions">Variable Naming Conventions</h6>
                                <div class="mathquill"> x_t: state{\space}x{\space}at{\space}time{\space}t</div>
                                <div class="mathquill"> z_t: measurement{\space}obtained{\space}at{\space}time{\space}t{\space}(sensor)</div>
                                <div class="mathquill"> u_t: represents{\space}change{\space}of{\space}state{\space}that{\space}occurred{\space}between{\space}time{\space}t-1{\space}and{\space}t</div>
                                <br />

                                <img src="img/loc9.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />

                                <h4>Measurement Update</h4>
                                <br />
                                <h5>Mean Calculation</h5>
                                <br />

                                <p>Let's suppose our robot's current position is near the 20m mark but we are not incredibly certain so the Prior Belief Gaussian has a rather wide probability distribution. Next our robot takes its first sensory measurement
                                    providing with data to work with. The measurement data <span class="mathquill">z</span> is more certain so it appears a narrow Gaussian with a mean of 30. </p>
                                <img src="img/loc10.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />
                                <p><b>Given a prior belief about our robot's state and the measurement that it collected where do we think the robot's new belief will be?</b></p>
                                <p>The new belief would have a mean somewhere inbetween the two Gaussians since it is combining information from both of them. Since the measurement update is more certain, the new mean would lie closer to the measurement
                                    update.</p>
                                <p>The new mean is a weighted sum of the prior belief and measurement means. With uncertainty, a larger number represents a more uncertain probability distribution. However, the new mean should be biased towards the measurement
                                    update, which has a smaller standard deviation than the prior. How do we accomplish this?</p>
                                <div class="mathquill">\large \mu' = \frac{r^2 \mu + \sigma^2 v}{r^2 + \sigma^2}</div>
                                <br />
                                <p>The answer is - the uncertainty of the prior is multiplied by the mean of the measurement, to give it more weight, and similarly the uncertainty of the measurement is multiplied with the mean of the prior. Applying this
                                    formula to our example generates a new mean of 27.5, which we can label on our graph below.</p>

                                <h5>Variance Calculation</h5>
                                <br />
                                <p>Next, we need to determine the variance of the new state estimate.</p>
                                <p><b>Would the variance of the new state estimate be less confident than our prior, in between our prior and measurement, or more confident than our measurement?</b></p>
                                <p>The two Gaussians provide us with more information together than either Gaussian offered alone. As a result, our new state estimate is more confident than our prior belief and our measurement. This means that it has a higher
                                    peak and is narrower.</p>
                                <img src="img/loc11.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />

                                <p>The formula for the new variance is presented below. </p>
                                <div class="mathquill">\large \sigma^{2'} = \frac{1}{\frac{1}{r^2} + \frac{1}{\sigma^2}}</div>
                                <br />
                                <p>Entering the variances from our example into this formula produces a new variance of 2.25. The new state estimate, often called the <strong>posterior</strong>, is drawn above.</p>

                                <img src="img/loc12.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />

                                <h4>State Prediction</h4>
                                <br />

                                <p>State Prediction is the estimation that takes place after an inevitably uncertain motion.</p>
                                <img src="img/loc13.png" alt="" style="width:60%" class="img img-fluid" />
                                <br />
                                <p>Since the measurement update and starte prediction are iterative cycle, it makes sense for us to continue where we left off. After taking into account the measurement, the posterior distribution was a gaussian with a mean
                                    of 27.5 and a variance of 2.25. However, simce we moved on to the state prediction step in the Kalman Filter cycle, this gaussian is now referred to as the <strong>Prior Belief</strong>. This is the robot's best estimate
                                    of its current location.</p>
                                <p>Next, the robot executes a command, move forward 7.5m. The result of this motion is a gaussian distribution centered around 7.5m with a variance of 5m. Calculating the new estimate is as easy as adding the mean of the motion
                                    to the mean of the prior. Similarly, adding the variances together to produce the posterior gaussian. </p>

                                <div class="mathquill">Posterior\ Mean \ \ \ \ \ \ \ \large \mu' = \mu_1 + \mu_2</div>
                                <div class="mathquill">Posterior\ Variance \ \ \ \ \large \sigma^{2'} = \sigma^2_1 + \sigma^2_2</div>
                                <br />

                                <img src="img/loc14.png" alt="" style="width:60%" class="img img-fluid" />
                                <!-- <br /> -->


                                <!-- <div class="row">
                                  <div class="column">
                                    <img src="img/td1.png" alt="td1" style="width:100%">
                                  </div>
                                  <div class="column">
                                    <img src="img/td2.png" alt="td2" style="width:100%">
                                  </div>
                                  <div class="column">
                                    <img src="img/td3.png" alt="td3" style="width:100%">
                                  </div>
                                </div> -->
                        </div>
                        <div class="divider"></div>
                        <div class="ud-atom">
                            <h3></h3>
                            <div>
                                <h2 id="-Multivariate-Gaussians">Multivariate Gaussians</h2>
                                <p>Most robots that we would be interested in modeling are moving in more than one dimension. For instance, a robot on a plane would have an x &amp; y position. </p>
                                <p>The simple approach to take, would be to have a 1-dimensional Gaussian represent each dimension - one for the x-axis and one for the y-axis.</p>
                                <p>Do you see any problems with this? Why couldn't we use multiple 1-dimensional Gaussians to represent multi-dimensional systems? Because there may be correlations between dimensions that we would not be able to model by
                                    using independent 1-dimensional Gaussians.</p>

                                <h4 id="-2-Dimensional-Gaussian">2-Dimensional Gaussian</h4>

                                <div class="row">
                                    <div class="column">
                                        <img src="img/multivariate-gaussian.png" alt="multivariate-gaussian" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc15.png" alt="loc15" style="width:100%">
                                    </div>
                                </div>
                                <br />

                                <h5 id="mean">Mean</h5>
                                <p>The mean is now a vector,</p>
                                <div class="mathquill"> \large \mu = \left[ \begin{array}{c} \mu_{x} \\ \mu_{y} \end{array} \right] </div>
                                <br />

                                <h5 id="covariance">Covariance</h5>
                                <p>The multidimensional equivalent of variance is a covariance matrix,</p>
                                <div class="mathquill"> \large \Sigma = \left[ \begin{array}{cc} \sigma_{x}^2 & \sigma_{x}\sigma_{y} \\ \sigma_{y}\sigma_{x} & \sigma_{y}^2 \end{array} \right]
                                </div>
                                <br />

                                <p>The covariance matrix represents the spread of the gaussian in 2-D. The diagonal of the matrix (<span class="mathquill ud-math"> \sigma_{x}^2</span> and <span class="mathquill ud-math"> \sigma_{y}^2</span>) represent the
                                    variances, while the off-diagonal elements (<span class="mathquill ud-math"> \sigma_{y}\sigma_{x}</span> and <span class="mathquill ud-math"> \sigma_{x}\sigma_{y}</span>) represents correlation terms. These terms are
                                    non-zero if there is a correlation between the variance in one dimension and the variance in another. When that is the case, the Gaussian function looks 'skewed' when looked at from above. </p>

                                <p>An N-D (N-Dimensional) Gaussian would have a covariance matrix that is of size N x N.

                                    <p>An important fact to note is that the covariance matrix is always symmetrical, i.e., <span class="mathquill">\sigma_{x}\sigma_{y} = \sigma_{y}\sigma_{x}</span></p>

                                    <p>To make better sense of what a covariance matrix represents, let's take a look at few examples of 2-D Gaussians. </p>
                                    <div class="row">
                                        <div class="column">
                                            <img src="img/loc16.png" alt="loc16" style="width:100%">
                                        </div>
                                        <div class="column">
                                            <p>If X and Y variances are small and equal to each other, then the 2-D Gaussian representation would be circular. In such a case, we are equally certain about the location of an object in each dimension.</p>
                                        </div>
                                    </div>
                                    <br />
                                    <div class="row">
                                        <div class="column">
                                            <img src="img/loc17.png" alt="loc17" style="width:100%">
                                        </div>
                                        <div class="column">
                                            <p>However, it's possible that we are quite certain about the location of an object in X-axis but not so much in Y-axis. In such a case, the contour lines of Gaussian would start to take on an oval shape.</p>
                                        </div>
                                    </div>
                                    <br />
                                    <div class="row">
                                        <div class="column">
                                            <img src="img/loc18.png" alt="loc18" style="width:100%">
                                        </div>
                                        <div class="column">
                                            <p>In the previous case, the correlation terms are 0. If the correlation terms are non-zero, then the two axes are correlated and we would have skewed oval. Depending on whether the terms multiplied out to a +ve
                                                value or -ve value, our oval would skewed to right or left respectively. Then once we gain information about one axis, it would reveal information about the other due to the correlation.</p>
                                        </div>
                                    </div>
                                    <br />
                                    <p>A Multivariate Gaussian can be modelled using the following formula: </p>
                                    <div class="mathquill"> \large p(x) = \frac{1}{(2\pi)^{\frac{D}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}</div>
                                    <p>where D represents the number of dimensions present. If D=1, the formula simplifies to the formula for the one-dimensional Gaussian that we have seen before.</p>
                                    <p><span class="mathquill ud-math"> x</span> and <span class="mathquill ud-math"> \mu</span> are vectors, and <span class="mathquill ud-math"> \Sigma</span> is a matrix. </p>
                                </p>
                            </div>
                        </div>
                        <div class="divider"></div>
                        <div class="ud-atom">
                            <h3></h3>
                            <div>
                                <h2 id="-Multidimensional-KF">Multidimensional Kalman Filters</h2>

                                <p>In our 1-D examples, the system state was represented by one variable. In n-dimensional systems, the state is a vector with n state variables. </p>
                                <p><b>Example: </b>In a 2-D world, states could be
                                <span class='mathquill'> \large  
                                  \begin{bmatrix}
                                  x \\
                                  y
                                  \end{bmatrix}
                                </span> (x and y positions of a robot), or

                                <span class='mathquill'> \large  
                                  \begin{bmatrix}
                                  x \\
                                  \dot{x}
                                  \end{bmatrix}
                                </span> (position and velocity of a robot)</p>
                                <p>When working in 1-D, the state has to observable, meaning it has to be something that can be measured directly.</p>
                                <p>In multidimensional states, there may exist hidden state variables, ones that we can't measure with the sensors available. However, we may be able to infer their values from other states and measurements. In the example
                                <span class='mathquill'> \large  
                                  \begin{bmatrix}
                                  x \\
                                  \dot{x}
                                  \end{bmatrix}
                                </span>, the location of the robot is observable while its velocity is not, making it a hidden state variable. However, a robot's position and velocity over time are linked through a very simple formula: <span class="mathquill">\large x' = x + \dot{x} \Delta t </span></p>
                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc19.png" alt="loc19" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc20.png" alt="loc20" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc21.png" alt="loc21" style="width:100%">
                                    </div>
                                </div>
                                <p>Let's look at this on a graph! If initially the position of the robot is known but its velocity is not, then the estimate of the robot's state would like as shown above. A gaussian that is very narrow in the <span class="mathquill">\large x</span>                                    dimension, representing confidence about robot's location and very wide along <span class="mathquill">\large \dot{x}</span> dimension since the robot's velocity is completely unknown. Next, a <strong>state prediction</strong>                                    can be calculated. This is where a Kalman Filter starts to get exciting. Knowing the relationship between a hidden variable and observable variable is key. </p>
                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc22.png" alt="loc22" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc23.png" alt="loc23" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc24.png" alt="loc24" style="width:100%">
                                    </div>
                                </div>
                                <p>Let's assume one iteration of a kalman Filter taskes 1s, i.e., <span class="mathquill">(\Delta t = 1)</span>. Now using the above formula we can calculate the posterior state for each possible velocity. For example, if
                                    the velocity is 0, the posterior state would be identical to the prior. Similarly, put different values of velocity in above formula to get different posterior states. From this, we can draw a posterior gaussian like
                                    shown above. This doesn't yet tell us anything about the velocity. It just graphs the correlation between velocity and the location of the robot.</p>
                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc25.png" alt="loc25" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc26.png" alt="loc26" style="width:100%">
                                    </div>
                                </div>
                                <p>Next is a <strong>measurement update</strong>. Let's see if it can provide us with sufficient information to identify the velocity of the robot.
                                    <br /> The initial belief was useful to calculate the state prediction but has no additional value and the result of the state prediction can be called prior belief for our measurement update.
                                    <br />Let's say the new measurement suggests a new location of the robot (like <span class="mathquill">x=</span>50). If we apply measurement update to the new prior, we will have a very small posterior, centered around
                                    <span
                                    class="mathquill">x=</span>50 and <span class="mathquill">\dot{x}=</span>15. It's the exact same measurement update we applied before, implemented in 2-D. Check the second graph above. <strong>The posterior belief is just the weighted sum of the prior belief and the measurement and more confident than either the prior or the measurement.</strong>                                        Then the relationship between the two dimension narrows down to posterior for the <span class="mathquill">\dot{x}</span> axis. After all, if the robot moved from <span class="mathquill">x=</span>35 to <span class="mathquill">x=</span>50
                                        in 1s, the speed should be trivial to calculate.</p>

                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc27.png" alt="loc27" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc28.png" alt="loc28" style="width:100%">
                                    </div>
                                </div>
                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc29.png" alt="loc29" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc30.png" alt="loc30" style="width:100%">
                                    </div>
                                </div>

                                <p>So, it seems the two iterations of the KF cycle were enough to infer the robot's velocity. Continuing iteration through the measurement update and state prediction steps will update the robot's internal state to keep it
                                    aligned with where it is in the real world.</p>


                                <h4 id="-Design-of-Multi-Dimensional-KF">Design of Multi-Dimensional Kalman Filters</h4>

                                <h5 id="-State-Transition">State Transition</h5>
                                <p>The formula below is the state transition function that advances the state from time <em>t</em> to time <em>t + 1</em>. It is just the relationship between the robot’s position, <span class="mathquill ud-math">x</span>,
                                    and velocity, <span class="mathquill ud-math">\dot{x}</span>. Here, we will assume that the robot’s velocity is not changing.</p>
                                <div class="mathquill">\large x' = x + \Delta t \dot{x}</div>
                                <div class="mathquill">\large \dot{x}' = \dot{x}</div>
                                <p>We can express the same relationship in matrix form, as seen below. On the left, is the posterior state (denoted with the prime symbol, <span class="mathquill ud-math">'</span>), and on the right are the state transition
                                    function and the prior state. This equation shows how the state changes over the time period, <span class="mathquill ud-math">\Delta t</span>. Note that we are only working with the means here; the covariance matrix
                                    will appear later.</p>
                                <div class='mathquill'> \large \begin{bmatrix} x \\ \dot{x} \end{bmatrix}' = \begin{bmatrix} 1 & \Delta{t} \\ 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ \dot{x} \end{bmatrix}
                                </div>
                                <br />

                                <p>The State Transition Function is denoted <span class="mathquill ud-math">F</span>, and the formula can be written as so,</p>
                                <div class="mathquill">\large x' = Fx</div>
                                <p>In reality, the equation should also account for process noise, as its own term in the equation. However, process noise is a Gaussian with a mean of 0, so the update equation for the mean need not include it. </p>
                                <div class="mathquill">\large x' = Fx + noise</div>
                                <div class="mathquill">\large noise \sim N(0,Q)</div>
                                <p>Now, what happens to the covariance? How does it change in this process? </p>
                                <p><strong>Sidenote:</strong> While it is common to use <span class="mathquill ud-math">\Sigma</span> to represent the covariance of a Gaussian distribution in mathematics, it is more common to use the letter <span class="mathquill ud-math">P</span>                                    to represent the state covariance in localization.</p>
                                <p>If you multiply the state, <span class="mathquill ud-math">x</span>, by <span class="mathquill ud-math">F</span>, then the covariance will be affected by the square of <span class="mathquill ud-math">F</span>. In matrix
                                    form, this will look like so:</p>
                                <div class="mathquill">\large P' = FPF^T</div>
                                <p>However, your intuition may suggest that it should be affected by more than just the state transition function. For instance, additional uncertainty may arise from the prediction itself. If so, you’re correct! </p>
                                <p>To calculate the posterior covariance, the prior covariance is multiplied by the state transition function squared, and <span class="mathquill ud-math">Q</span> added as an increase of uncertainty due to process noise.
                                    <span class="mathquill ud-math">Q</span> can account for a robot slowing down unexpectedly, or being drawn off course by an external influence. </p>
                                <div class="mathquill">\large P' = FPF^T + Q</div>
                                <p>Now we’ve updated the mean and the covariance as part of the state prediction. </p>

                                <br />
                                <h5 id="quiz-1">Quiz 1: </h5>
                                <p>You are tracking the position and velocity of a robot in two dimensions, x and y. The state is represented as so,</p>
                                <div class='mathquill'> \large x = \begin{bmatrix} p_x \\ p_y \\ v_x \\ v_y \end{bmatrix}
                                </div>
                                <p>Find the state update/transition function, F, that will advance the state from time <em>t</em> to time <em>t + 1</em> based on the state transition equation: <span class="mathquill">\large x' = Fx</span></p>
                                <summary><strong>SOLUTION:</strong></summary>
                                <div class="row">
                                  <div class="column">
                                      <div class='mathquill'> F = \begin{bmatrix} 1 & 0 & \Delta t & 0 \\ 0 & 1 & 0 & \Delta t \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} </div>
                                  </div>
                                  <div class="column">
                                      <div class="mathquill">\large p_x' = p_x + v_x \Delta t</div>
                                      <div class="mathquill">\large p_y' = p_y + v_y \Delta t</div>
                                  </div>
                                  <div class="column">
                                    <div class="mathquill">\large v_x' = v_x</div>
                                    <div class="mathquill">\large v_y' = v_y</div>
                                  </div>
                                </div>
                                <br />
                                <h5 id="quiz-2">Quiz 2:</h5>
                                <p>You are tracking the position, velocity, and acceleration of a quadrotor in the vertical dimension, z. The state of the quadrotor can be represented as so, </p>
                                <div class='mathquill'> \large x = \begin{bmatrix} z \\ \dot{z} \\ \ddot{z} \end{bmatrix}
                                </div>
                                <p>Find the state update function, F, that will advance the state from time <em>t</em> to time <em>t + 1</em> based on the state transition equation: <span class="mathquill">\large x' = Fx</span></p>
                                <summary><strong>SOLUTION:</strong></summary>
                                <div class="row">
                                  <div class="column">
                                      <div class='mathquill'> F = \begin{bmatrix} 1 & \Delta t & \frac{1}{2} \Delta t^2 \\ 0 & 1 & \Delta t \\ 0 & 0 & 1 \end{bmatrix} </div>
                                  </div>
                                  <div class="column">
                                      <div class="mathquill">\large z' = z + \dot{z} \Delta t + 1/2 \ddot{z} \Delta t²</div>
                                      <div class="mathquill">\large \dot{z'} = \dot{z} + \ddot{z} \Delta t</div>
                                      <div class="mathquill">\large \ddot{z'} = \ddot{z}</div>
                                  </div>
                                </div>
                                <br />
                                <div class="divider"></div>
                                <br />
                                <h5 id="-Measurement-Update">Measurement Update</h5>
                                <p>Next, we move onto the measurement update step. If we return to our original example, where we were tracking the position and velocity of a robot in the x-dimension, the robot was taking measurements of the location only
                                    (the velocity is a hidden state variable). Therefore the measurement function is very simple - a matrix containing a one and a zero. This matrix demonstrates how to map the state to the observation, <span class="mathquill ud-math">z</span>.</p>
                                <div class='mathquill'> \large z = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} x \\ \dot{x} \end{bmatrix}
                                </div>
                                <p>This matrix, called the Measurement Function, is denoted <span class="mathquill ud-math">H</span>.</p>
                                <p>For the measurement update step, there are a few formulas. First, we calculate the measurement residual, <span class="mathquill ud-math">y</span>. The measurement residual is the difference between the measurement and the
                                    expected measurement based on the prediction (ie. we are comparing where the measurement <em>tells us</em> we are vs. where we <em>think</em> we are). The measurement residual will be used later on in a formula.</p>
                                <div class="mathquill">\large y = z - Hx'</div>
                                <p>Next, it's time to consider the measurement noise, denoted <span class="mathquill ud-math">R</span>.</p> 
                                <div class="mathquill">\large S = HP'H^T + R</div>
                                <p>Above formula maps the state prediction covariance into the measurement space and adds the measurement noise. The result,
                                <span class="mathquill ud-math">S</span>, will be used in a subsequent equation to calculate the Kalman Gain.</p>
                                <br />
                                <div class="divider"></div>
                                <br />
                                <h5 id="-Kalman-Gain">Kalman Gain</h5>
                                <p>Next, we calculate the Kalman Gain, K. As you will see in the next equation, the Kalman Gain determines how much weight should be placed on the state prediction, and how much on the measurement update. It is an averaging
                                    factor that changes depending on the uncertainty of the state prediction and measurement update. </p>
                                <div class="mathquill">\large K = P'H^TS^{-1}</div>
                                <div class="mathquill">\large x = x' + Ky</div>
                                <p>These equations may look complicated and intimidating, but they do nothing more than calculate an average factor.</p>
                                <br />
                                <div class="divider"></div>
                                <br />

                                <p><strong>Two extreme cases for the Kalman Filter:</strong></p>
                                <ul>
                                  <li><strong>Case:</strong> Sensor we are measuring with is perfectly accurate, then the measurement noise <span class="mathquill">R</span> would be a matrix of 0s, i.e., </li><br />
                                  <div class="row">
                                    <div class="column">
                                        <img src="img/loc32.jpg" alt="loc32" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc32-1.png" alt="loc32-1" style="width:100%">
                                    </div>
                                  </div><br />
                                  <p><strong>The Kalman Gain is equal to the inverse of the measurement function, <span class="mathquill">H</span>.</strong></p>
                                  <p>Next step is <em>updating the mean using the Kalman Gain</em>.</p>
                                  <div class="row">
                                    <div class="column">
                                        <img src="img/loc31.jpg" alt="loc31" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc31-1.png" alt="loc31" style="width:100%">
                                    </div>
                                  </div><br />
                                  <p>The new mean is entirely made of the measurement. We have thrown out any information that state prediction provided because it's not needed. The measurement is perfect!</p>
                                  <li><strong>Case:</strong> Negligible sensor with utterly <strong>imperfect measurements</strong>, one that we know we cannot rely on.</li><br />
                                  <p>In such a case, <span class="mathquill">R</span> would be a matrix of <span class="mathquill">∞</span>s, i.e.,</p>
                                  <div class="row">
                                    <div class="column">
                                        <img src="img/loc33.jpg" alt="loc33" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <img src="img/loc33-1.png" alt="loc33-1" style="width:100%">
                                    </div>
                                  </div><br />
                                  <p>The new mean is equal to the state prediction and the measurement update is not included at all as it is unreliable.</p>
                                </ul><!-- <br /> -->
                                <div class="row">
                                    <div class="column">
                                        <img src="img/loc34.png" alt="loc33" style="width:100%">
                                    </div>
                                    <div class="column">
                                        <p>The two extremes for the Kalman Filter are a perfect sensor and a negligible sensor. In these two cases, the new state is either exclusively dependent on measurement update or exclusively reliant on the state prediction. In reality, we will more often find the Kalman Gain is somewhere in between the two. It's an averaging factor evaluating which of the measurement update or state prediction should be trusted more based on the measurement noise and the process noise.</p>
                                    </div>
                                  </div><br />
                                <div class="divider"></div><br />
                                <p>The last step in the Kalman Filter is to update the new state’s covariance using the Kalman Gain.</p>
                                <!-- <br /> -->
                                <div class="mathquill">\large P = (I - KH)P'</div>
                                <br />
                                <h2 id="kalman-filter-equations">Kalman Filter Equations</h2>
                                <p>These are the equations that implement the Kalman Filter in multiple dimensions.</p>
                                <p>State Prediction:</p>
                                <div class="mathquill">\large x' = Fx</div>
                                <div class="mathquill">\large P' = FPF^T + Q</div>
                                <p>Measurement Update:</p>
                                <div class="mathquill">\large y = z - Hx'</div>
                                <div class="mathquill">\large S = HP'H^T + R</div>
                                <p>Calculation of Kalman Gain:</p>
                                <div class="mathquill">\large K = P'H^TS^{-1}</div>
                                <p>Calculation of Posterior State and Covariance:</p>
                                <div class="mathquill">\large x = x' + Ky</div>
                                <div class="mathquill">\large P = (I - KH)P'</div>
                                <p>The Kalman Filter can successfully recover from inaccurate initial estimates, but it is very important to estimate the noise parameters, Q and R, as accurately as possible - as they are used to determine which of the estimate or the measurement to believe more.</p>
                            </div>
                        </div>

                        <!-- <div class="divider"></div> -->
                        <div class="divider"></div>

                    </div>
            </main>

            <footer class="footer">
                <div class="container">
                    <div class="row">
                        <div class="col-12">
                            <p class="text-center">
                                Copyright &copy 2024. This work is licensed under a <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
                            </p>
                        </div>
                    </div>
                </div>
            </footer>

            <script src="../../assets/js/jquery-3.3.1.min.js"></script>
            <script src="../../assets/js/plyr.polyfilled.min.js"></script>
            <script src="../../assets/js/bootstrap.min.js"></script>
            <script src="../../assets/js/jquery.mCustomScrollbar.concat.min.js"></script>
            <script src="../../assets/js/katex.min.js"></script>
            <script>

                // render math equations
                let elMath = document.getElementsByClassName('mathquill');
                for (let i = 0, len = elMath.length; i < len; i += 1) {
                  const el = elMath[i];
            
                  katex.render(el.textContent, el, {
                    throwOnError: false
                  });
                }
            
                // this hack will make sure Bootstrap tabs work when using Handlebars
                if ($('#question-tabs').length && $('#user-answer-tabs').length) {
                  $("#question-tabs a.nav-link").on('click', function () {
                    $("#question-tab-contents .tab-pane").hide();
                    $($(this).attr("href")).show();
                  });
                  $("#user-answer-tabs a.nav-link").on('click', function () {
                    $("#user-answer-tab-contents .tab-pane").hide();
                    $($(this).attr("href")).show();
                  });
                } else {
                  $("a.nav-link").on('click', function () {
                    $(".tab-pane").hide();
                    $($(this).attr("href")).show();
                  });
                }
            
                // side bar events
                $(document).ready(function () {
                  $("#sidebar").mCustomScrollbar({
                    theme: "minimal"
                  });
            
                  $('#sidebarCollapse').on('click', function () {
                    $('#sidebar, #content').toggleClass('active');
                    $('.collapse.in').toggleClass('in');
                    $('a[aria-expanded=true]').attr('aria-expanded', 'false');
                  });
                });
            </script>
</body>

</html>